The image is a logo for "IEEE Access." The text is in two colors: "IEEE" is in dark blue, and "Access" is in light blue. Below the main text, there are three descriptors separated by bullet points: "Multidisciplinary," "Rapid Review," and "Open Access Journal." The font is bold and modern, and there is a registered trademark symbol (®) next to "Access." The overall design is clean and professional.

Received February 14, 2020, accepted March 2, 2020, date of publication March 5, 2020, date of current version March 18, 2020.

Digital Object Identifier 10.1109/ACCESS.2020.2978804

Galaxy Image Classification Based on Citizen
Science Data: A Comparative Study

MANUEL JIMÉNEZ
1, MERCEDES TORRES TORRES
2, ROBERT JOHN
1, (Senior Member, IEEE),
AND ISAAC TRIGUERO
1, (Member, IEEE)
1Computational Optimisation and Learning Laboratory (COL), School of Computer Science, University of Nottingham, Nottingham NG8 1BB, U.K.
2Computer Vision Laboratory (CVL), School of Computer Science, University of Nottingham, Nottingham NG8 1BB, U.K.

Corresponding author: Manuel Jiménez (manuel.jimenezmorales@nottingham.ac.uk)

This work was supported in part by the NVIDIA Corporation, and in part by the Titan Xp GPU. The work of Manuel Jiménez was
supported by the School of Computer Science, University of Nottingham, through the Ph.D. Scholarship.

ABSTRACT Many research ﬁelds are now faced with huge volumes of data automatically generated by
specialised equipment. Astronomy is a discipline that deals with large collections of images difﬁcult to
handle by experts alone. As a consequence, astronomers have been relying on the power of the crowds,
as a form of citizen science, for the classiﬁcation of galaxy images by amateur people. However, the new
generation of telescopes that will produce images at a higher rate highlights the limitations of this approach,
and the use of machine learning methods for automatic classiﬁcation is considered essential. The goal of
this paper is to shed light on the automated classiﬁcation of galaxy images exploring two distinct machine
learning strategies. First, following the classical approach consisting of feature extraction together with a
classiﬁer, we compare the state-of-the-art feature extractor for this problem, the WND-CHARM, with our
proposal based on autoencoders for feature extraction on galaxy images. We then compare these results with
an end-to-end classiﬁcation using convolutional neural networks. To better leverage the available citizen
science data, we also investigate a pre-training scheme that exploits both amateur- and expert-labelled data.
Our experiments reveal that autoencoders greatly speed up feature extraction in comparison with WND-
CHARM and both classiﬁcation strategies, either using convolutional neural networks or feature extraction,
reach comparable accuracy. The use of pre-training in convolutional neural networks, however, has allowed
us to provide even better results.

INDEX TERMS
Astroinformatics, autoencoders, citizen science, convolutional neural networks, deep learning, feature
extraction, galaxy morphologies, image classiﬁcation.

I. INTRODUCTION
Classiﬁcation is one of the core tasks addressed by machine
learning (ML) algorithms [1], [2]. A classiﬁer is usually
trained to learn patterns from input data, aiming to predict the
label to be assigned to previously unseen data instances [3].
In image classiﬁcation, we pursue the categorisation of
images into two or more classes, being either mutually exclu-
sive (multi-class classiﬁcation, in which only one single
class is assigned) or not (multi-label classiﬁcation, where
different classes coexist). These paradigms are widely imple-
mented in multiple real-world applications such as ﬁngerprint
identiﬁcation [4] or the recognition of facial emotions [5],

The associate editor coordinating the review of this manuscript and

approving it for publication was Shun-Feng Su
.

and they have also become a useful tool for data analysis in
science and engineering [6], [7].
Astronomers have seen their data processing capabilities
exceeded with the advent of modern instrumentation [8],
leading to the emergence of the astroinformatics disci-
pline [9] to help analyse the data provided. In most cases,
this entails the classiﬁcation of large collections of astro-
nomical images [10]–[12]. Particularly, the morphological
classiﬁcation of galaxy images aims at the categorisation of
these objects into two main classes (morphologies), elliptical
and spiral [13]. The morphology is a key indicator for under-
standing the galaxy inner structure and physical processes,
also revealing aspects about the formation and evolution of
the universe [14]. However, due to the huge amounts of
images produced in modern telescopes [15], astronomers

47232
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
VOLUME 8, 2020

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

which we also hold expert classiﬁcations for a subset of ∼41k
examples. We ﬁrst explore the inﬂuence in the results of
several factors such as the image size or the presence of colour
channels using the subset with expert classiﬁcations. We then
investigate the scalability of both classiﬁcation approaches
in the larger dataset. Finally, we utilise the whole GZ1 data
with both label sets to pre-train the CNNs using amateur
classiﬁcations and then ﬁne-tune them on the expert-labelled
subset.
The rest of the paper is structured as follows. In Section II,
we brieﬂy introduce related work about the classiﬁcation of
galaxy images with citizen science and the ML approaches
taken for this problem to date. Section III presents our pro-
posed models of AEs and CNN for the FE and classiﬁcation
of galaxy images, respectively, including a brief explanation
of the proposed pre-training approach with citizen science
data. Then, in Section IV we explain the experimental setup
established. Section V presents the results and discussion, and
ﬁnally Section VI concludes the paper.

II. RELATED WORK
This section provides information about the central concepts
of the paper. First, we describe in more detail the classiﬁ-
cation of galaxy images with citizen science (Section II-A).
After this, we present related work about ML approaches for
the automatic classiﬁcation of galaxy images (Section II-B).

have been drawing upon the general public for this task using
the internet, giving rise to the re-emergence of the citizen
science movement [16], [17]. This was ﬁrst materialised
with the release of the Galaxy Zoo 1 (GZ1) project [18],
which generated the largest manually annotated catalogue of
galaxy images to date [19]. Nonetheless, the next generation
of astronomical surveys that will produce billions of galaxy
images [20] shows the limitations of this approach. ML meth-
ods are needed, pursuing a robust automation of the classiﬁ-
cation task, and several efforts have recently been developed
in this direction [21]–[24].
The traditional ML approach to image classiﬁcation
requires the extraction of features from the image. Classical
learning algorithms (e.g. Decision Trees, k-Nearest Neigh-
bours) do not cope well with images directly, which is typi-
cally solved transforming the image pixels into a new feature
space by means of feature extraction (FE) techniques [25].
In contrast, deep learning (DL) based approaches [26] need
minor or no data preprocessing for the classiﬁcation of
images. More speciﬁcally, convolutional neural networks
(CNNs) [27], [28] provide excellent solutions [29], being
capable of taking a raw image as input and perform an
implicit FE process along with the classiﬁcation in one single
step. Recent state-of-the-art CNNs are usually composed of
a very large number of layers [30] when dealing with chal-
lenging image classiﬁcation problems [31]. Alternatively,
DL can also be used to extract features of an image by
means of autoencoders (AEs) [32], which have also been
proposed to ease the learning of standard classiﬁers [33].
Whereas CNNs often need to learn the image features from
scratch using a large amount of labelled data, AEs enable
the encapsulation of the FE process for a particular problem
without any need of labels, which can be advantageous for
the classiﬁcation of big collections of images and the use of
other kind of classiﬁers [33].
The classiﬁcation of galaxy images has leveraged both
general strategies [34]. However, the variable characteristics
of the images commonly used in the training of ML for this
problem have systematically neglected a fair comparison of
different methods under the same learning conditions. This
work presents a comparative study of distinct approaches for
galaxy image classiﬁcation, investigating their advantages
and disadvantages in a common experimental framework.
Following the classical approach, we explore the suitability
of two feature extractors. On the one hand, we take the WND-
CHARM multi-purpose feature extractor [35] as the state-of-
the-art FE method [22], [36], [37] used in this problem. On the
other hand, we propose two AE architectures for the FE of this
kind of images, which, to the best of our knowledge, has not
been explored yet. We also analyse the effect of two feature
selection methods on the resulting feature sets, and then com-
pare these results with an end-to-end approach using CNNs.
Here we propose a simple yet effective CNN architecture and
compare it with a deeper CNN, namely ResNet [31].
The experiments are carried out over the GZ1 main dataset,
consisting of nearly 668k images annotated by amateurs for

A. GALAXY IMAGE CLASSIFICATION AND CITIZEN
SCIENCE
Here we consider the morphological classiﬁcation of galaxy
images, that is, the classiﬁcation of these objects according
to a blend of the galaxy shape, colour, and texture [38].
This has been standard practice since it was ﬁrst applied
by E. Hubble nearly a century ago [13]. The morphology
provides a ﬁrst-order descriptor about the galaxy, which is
key for astronomers in the study of fundamental questions
about their inner physics [39], interactions [40], or evolu-
tion [41]. There are two main morphological types based on
the presence or not of a disk: spiral and elliptical, respectively.
However, the multiplicity of hybrid types and the wide range
of image conditions depending on factors such as the galaxy
brightness, size or distance, turn the classiﬁcation of this sort
of images into a very complex task.
Citizen science has been a partial solution for this problem,
with the engagement of amateur people from the general pub-
lic in this kind of data analysis [42]. Citizen science projects
join the endeavours of myriads of volunteers committed to
helping with a task that typically is time consuming and
tedious for experts, but also decisive for getting advances
in a certain research problem. A task usually covered is the
classiﬁcation of images, for which the Galaxy Zoo project
represents the most successful implementation to date [43].
Its ﬁrst edition, the Galaxy Zoo 1 (GZ1) [18], was focused
on the distinction among spiral and elliptical morphologies,
providing amateur classiﬁcations for nearly 900k galaxy
images. In addition, there are also expert classiﬁcations for a

VOLUME 8, 2020
47233

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

subset of ∼41k of the GZ1 images, as it is explained in [18].
This inclusion of amateurs in research tasks has brought
additional uncertainty into results. Nonetheless, given the
potential of this large-scale data processing from the ML
perspective, solutions are being proposed to overcome this
issue [44].
Along with numerous scientiﬁc insights,1 GZ1 and the
subsequent releases of the project have also generated enough
labelled data for the proper training of ML algorithms [19],
[45], [46]. However, ML implementations using this data
have generally aimed at replicating participants’ classiﬁca-
tion skills [21], [23] instead of tackling the uncertainty in
the results and/or taking advantage of available expert clas-
siﬁcations [18]. In this work, we are interested in exploring
how the inclusion of amateur labels affects the classiﬁcation
results, leveraging this particular characteristic featured by
the GZ1 dataset.

WND-CHARM represents the state-of-the-art feature extrac-
tor for the classiﬁcation of galaxy images [22], [36], [37].
First, through a FE phase, it computes a set of families of
features from the raw images in a one by one fashion. These
are categorised as image content descriptors, image trans-
forms, and compound image transforms (transformations of a
previous image transformation), composing a feature vector
for the image at hand [54]. Depending on the presence or
not of colour in the image, two feature sets are available.
Then, a feature selection (FS) phase chooses the most infor-
mative subset by calculating the Fisher discriminant score of
each feature. Finally, the resulting feature vectors are classi-
ﬁed using a modiﬁed nearest neighbour (NN) rule [55] that
weights the distances using the Fisher scores previously com-
puted. Whereas in traditional NN only the closest (or k clos-
est) examples determine the class, with WND-CHARM the
distances to all training samples of each class are measured.
In this work, we only consider the FE and FS phases of the
WND-CHARM, the so-called WND-CHARM feature map,
for the comparison of this method with the FE performed with
AEs.

B. MACHINE LEARNING STRATEGIES FOR GALAXY IMAGE
CLASSIFICATION
This section presents an overview of works found in the
specialised literature concerning the use of ML for galaxy
image classiﬁcation. First, FE based approaches are reviewed
(Section II-B.1). Then, we brieﬂy introduce CNNs and their
latest trends, and we examine their use in galaxy image
classiﬁcation (Section II-B.2).

1) FEATURE EXTRACTION PLUS A CLASSIFIER
FE methods used in the classiﬁcation of galaxy images can
be grouped into two main categories: problem speciﬁc, which
have been especially devised for this particular problem, and
general, which cope with image classiﬁcation regardless of
the problem deﬁnition. Among the ﬁrst, we review the use
of physical parameters extracted from the image, whereas
the second category is dominated by the WND-CHARM
feature set.
The classiﬁcation of galaxy images with ML started with
the extraction of a reduced number of physical parameters
from the image [47], [48]. These parameters accounted for
properties such as galaxy ellipticity, surface brightness, or
concentration. As a form of image features, they were then
classiﬁed using artiﬁcial neural networks [49] (ANNs). The
feature set was then extended to a greater number of parame-
ters and standardised with the so-called CAS (Concentration-
Asymmetry-Smoothness) methods [50], [51]. ANNs were
generally used for the classiﬁcation, along with other gen-
eral classiﬁers such as random forests and support vector
machines [52], [53]. Nonetheless, with the improvement of
computational resources, general purpose feature extractors
that compute longer sets of features have outperformed these
ﬁrst attempts.
Although it was originally developed as an image anal-
ysis tool for the classiﬁcation of biological images [35],

1The complete list of peer-reviewed publications based on the Galaxy Zoo
project results is available at https://www.zooniverse.org/about/publications.

2) CONVOLUTIONAL NEURAL NETWORKS
CNNs have systematically outperformed the classical bench-
marks in image classiﬁcation in the last few years [31], [56],
thanks to the ease of access to big datasets and the improve-
ments in computational resources. This has also been the
case in galaxy image classiﬁcation, promoted by the wide
availability of astronomical surveys on the web [15]. Basi-
cally, CNNs are ANNs with many hidden layers that progres-
sively reach more abstract representations of the input data
by computing non-linear transformations. These layers are
generally one of these types: convolution layer, pooling layer,
and fully connected layer. Whereas convolution and pooling
layers build and shrink the feature maps, respectively, fully
connected layers try to learn the global information present
at the end of these processes [28].
In the recent literature, very deep networks with a large
number of layers have been investigated [56]. The network
depth is of crucial importance for challenging image classi-
ﬁcation problems [30]. Many deep neural network architec-
tures (which use hundreds of layers) such as ResNet [31],
ResNext [57] or HRnet [58], have provided an outstanding
performance in varied image datasets with multitude of dif-
ferent objects [59]. These complex architectures are usually
exploited in a pre-trained fashion [60], which saves computa-
tional efforts and allows different domains to take advantage
of their prediction capabilities when the scarcity of anno-
tated examples invalidates the training of such models from
scratch [61]–[63].
CNNs have also been widely used in the classiﬁcation
of galaxy images, showing the limitations of FE methods
when the classiﬁcation is not restricted to the two main
morphologies. One of the ﬁrst successful implementations
took place in the framework of a Kaggle competition,

47234
VOLUME 8, 2020

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

The image is a diagram of an autoencoder neural network, which is used for dimensionality reduction and feature learning. The diagram is divided into two main parts: the encoder and the decoder.

### Encoder:
1. **Input Image**: The left side of the diagram shows the input image, represented by a column of black circles.
2. **Hidden Layer (1)**: The input image is passed through a dense layer with 1024 neurons.
3. **Hidden Layer (2)**: The output from the first hidden layer is then passed through another dense layer with 512 neurons.
4. **Encoding**: The output from the second hidden layer is compressed into 256 features, represented by a column of red circles. This is the bottleneck layer of the network.

### Decoder:
1. **Dense Connections**: The 256 features are then passed through a series of dense layers, each with a number of neurons that increase as the data is reconstructed.
2. **Output**: The final output is a reconstructed version of the input image, represented by a column of gray circles.

### Key Features:
- **Dense Connections**: The diagram shows dense connections between layers, meaning each neuron in one layer is connected to every neuron in the next layer.
- **Encoding**: The process of compressing the input image into 256 features is highlighted in the center of the diagram.
- **Reconstruction**: The decoder's role is to reconstruct the input image from the 256 features.

This autoencoder architecture is commonly used for tasks such as image compression, denoising, and feature learning.

FIGURE 1. Architecture of the deep autoencoder (DAE) proposed.

TABLE 1. Topology of the DAE proposed.

the Galaxy Challenge,2 which aimed at classifying a sample
of ∼50,000 galaxies from the Galaxy Zoo 2 dataset [45].
The goal was to predict the participants answers to a set of
questions about morphological traits featured by the galaxy.
The winner CNN architecture [23] established a benchmark
for this problem that has been widely employed thereafter
[24], [64]. However, these models make use of datasets of
moderate size to make feasible their training and employ
larger resources in terms of computational means and
runtime.
In this work, we propose a simpler CNN architecture
to distinguish between the two main morphological types.
We test this model against a well-established deeper model,
ResNet [31], and explore how their performances are affected
by the number and size of the images as well as the presence
or not of colour channels. We then compare these results with
the classiﬁcation using FE plus classiﬁer, aiming to investi-
gate in which occasions the different approaches work better.
Additionally, we also explore the pre-training of both CNN
models considered by exploiting the availability of expert and
amateur labels within citizen science data, which to the best
of our knowledge has not been investigated before.

III. METHODOLOGY: GALAXY IMAGE CLASSIFICATION
WITH DEEP LEARNING
This section introduces the proposed AE architectures for
developing the FE of galaxy images, as well as the CNN
proposed for comparison of ML approaches for galaxy image
classiﬁcation. First, we give a brief introduction to AEs
and describe the two models used through the experiments
(Section III-A). Then, we present the used CNN architecture
(Section III-B). Finally, we provide some background about
the pre-training of CNNs and explain our novel approach to
make use of amateur and expert labels (Section III-C).

datasets of images. First, their training is unsupervised, which
is key due to the scarcity of reliable image labels. Addition-
aly, they enable the encapsulation of the FE phase for the
exploration of patterns in the data and large-scale storage and
management of astronomical images. However, to the best of
our knowledge, a comparative study of their use for FE on
galaxy images has not been accomplished.
In our comparative study, we implement two different AE
models that showed the best performance among a wide set of
topologies that were tested. The ﬁrst one uses fully connected
layers in a more classical approach, while the second imple-
ments a CNN-based architecture. These models are based on
architectures originally designed for the classiﬁcation of the
MNIST dataset [68].

• The Deep AE (DAE) model deploys the architecture of
a deep and undercomplete AE, that is, an AE with more
than one hidden layer and the encoding having a lower
dimensionality than the input [32]. It holds two fully
connected layers between input and encoding. Follow-
ing this, the reverse structure is deployed from encoding
to output layer, thus completing the symmetrical struc-
ture as it is shown in Figure 1. For this model, the encod-
ing dimension is the same regardless of the input image
size: 256 features. The activation function used in all
neurons is the rectiﬁed linear unit (ReLU) [69], except
for the last layer (output), which applies the sigmoid
function. The DAE architecture is speciﬁed in Table 1.

A. AUTOENCODERS FOR FEATURE EXTRACTION ON
GALAXY IMAGES
AEs are a common architecture in unsupervised DL,
the referred as encoder-decoder [65]. Basically, an AE is an
ANN able to build new encodings for some input by means
of a symmetrical structure of layers that tries to resemble
the input pattern to the output as closely as possible [32].
The middle layer (symmetry axis) represents the encoding,
which is found after a training process that does not make
any use of the data labelling. As with other ANNs, the set of
weights and activation function associated with every neuron
generate the outputs layer by layer. A loss function computes
the disagreement between input and reconstruction, which
is optimised using the stochastic gradient descent [66] in
conjunction with the back-propagation algorithm [67].
AEs have been proposed for diverse tasks related to fea-
ture fusion [32] and dimensionality reduction to facilitate
the learning of canonical classiﬁers [33]. In astronomy, AEs
show a great potential for the processing and storage of large

2https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge

• The Convolutional AE (CAE) model uses the mecha-
nism of CNNs for learning the encoding, implementing

VOLUME 8, 2020
47235

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

The image is a diagram of a Convolutional Autoencoder, which is a type of neural network used for unsupervised learning of efficient codings of input data. The diagram is divided into two main parts: the Encoder and the Decoder.

### Encoder:
1. **Input Image (RGB):** The process begins with an input image represented in RGB (Red, Green, Blue) format.
2. **Convolution Layer:** The input image passes through a convolution layer with 16 kernels, each of size 3x3. This operation helps in extracting features from the input image.
3. **Max Pooling Layer:** Following the convolution layer, a max pooling operation with a 2x2 window is applied. This reduces the spatial dimensions (width and height) of the feature maps, helping to make the representation more robust to translations.
4. **Second Convolution Layer:** The output from the first max pooling layer is then passed through another convolution layer with 8 kernels, each of size 3x3.
5. **Second Max Pooling Layer:** Another max pooling operation with a 2x2 window is applied, further reducing the spatial dimensions.

### Flattening and Reshaping:
- The output from the second max pooling layer is flattened into a 1D vector.
- This flattened vector is then reshaped to prepare it for the decoder.

### Decoder:
1. **First Convolution Layer:** The reshaped vector is passed through the first convolution layer in the decoder, which has 8 kernels of size 3x3.
2. **First Max Pooling Layer:** A max pooling operation with a 2x2 window is applied.
3. **Second Convolution Layer:** The output is then passed through another convolution layer with 16 kernels, each of size 3x3.
4. **Second Max Pooling Layer:** Another max pooling operation with a 2x2 window is applied.

### Output:
- The final output is the reconstructed input image. The decoder essentially reverses the operations of the encoder to reconstruct the original input image from the encoded representation.

### Summary:
The diagram illustrates the flow of data through a Convolutional Autoencoder, showing how the input image is transformed through convolution and pooling layers in the encoder, flattened, and then reconstructed back to the original image through similar operations in the decoder. This process is used for tasks like image denoising, compression, and feature learning.

FIGURE 2. Architecture of the convolutional autoencoder (CAE) proposed.

TABLE 3. Topology of the proposed CNN.

TABLE 2. Topology of the CAE proposed. The reverse structure is
deployed for the decoder, using the sigmoid function in the last layer.

convolution and pooling layers. The model proposed
deploys three pairs of convolution – pooling layers
from input to encoding. After the third pooling layer,
the resulting tensor is ﬂatten to obtain the encoding,
as Figure 2 shows. Unlike the DAE model, for CAE
the number of features depends on the input image size.
The ﬁrst convolution layer holds 16 kernels and the
remaining two hold 8 kernels. The receptive ﬁelds are
3 × 3 pixels size, and pooling layers implement max
pooling with 2×2 pixels windows. As in the DAE, ReLU
activation functions are used along the network except
in the output layer, which applies the sigmoid function.
The complete speciﬁcations are presented in Table 2.

connections and produce the network output, consisting of
two neurons (binary classiﬁcation). These output layer gives
the class probabilities, which are rounded to produce the ﬁnal
classes labels. ReLU activation functions are used through
the whole structure except for the output layer, which applies
the softmax function that enables us to obtain probability
distributions. The architecture proposed is shown in Figure 3,
and all speciﬁcations presented in Table 3.
Deeper architectures were tested prior to the experiments,
implementing up to six pairs of convolution – pooling lay-
ers and/or different numbers of feature maps. Nonetheless,
the improvement was marginal or even diminished the accu-
racy in the results. Consequently, we opted to select the
CAE’s topology presented above and compare this simpler
architecture against one of the state-of-the-art models more
widely used in computer vision. Particularly, we selected
the lighter implementation of ResNet, ResNet50 [70], which
is composed of ﬁfty layers to exploit the residual learning
blocks that characterise this DL approach [31].

B. CONVOLUTIONAL NEURAL NETWORK FOR GALAXY
IMAGE CLASSIFICATION
Pursuing a fair comparison with the classiﬁcation using the
two AE models introduced above, the CNN architecture pro-
posed here resembles the CAE topology presented in the
previous section. Hence, this consists of three consecutive
pairs of convolution – pooling layers, which performs the FE
phase, followed by three fully connected layers that complete
the features classiﬁcation. As with the CAE, the network
computes 16 feature maps in the ﬁrst convolution layer, and
then 8 in second and third convolution layers. Pooling layers
implement the max pooling using 2 × 2 pixels windows.
After this, two layers of 256 and 128 neurons hold dense

C. PRE-TRAINING WITH CITIZEN SCIENCE DATA
The ﬁnal stage of our comparative study involves the pre-
training and ﬁne-tuning of the proposed CNN and the ResNet
using citizen science data. By this, we aim to investigate
the learning of such models from both amateur and expert

47236
VOLUME 8, 2020

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

The image is a diagram of a Convolutional Neural Network (CNN) architecture, illustrating the flow of data through various layers. Here's a detailed description:

1. **Input Image**:
- On the left, there is an input image, which is a small, grayscale image with a bright spot in the center.

2. **Convolutional Layer (C1)**:
- The input image is passed through a Convolutional layer labeled "C1" with 16 kernels (filters) of size 3x3.
- This layer produces 16 feature maps, each highlighting different features in the input image.

3. **Pooling Layer (P1)**:
- The output from the Convolutional layer is then passed through a Pooling layer labeled "P1" with a 2x2 window.
- The Pooling operation reduces the spatial dimensions of the feature maps, retaining the most important information.

4. **Convolutional Layer (C2)**:
- The pooled feature maps are then fed into another Convolutional layer labeled "C2" with 8 kernels of size 3x3.
- This layer produces 8 feature maps, capturing more abstract features.

5. **Pooling Layer (P2)**:
- The output from the second Convolutional layer is passed through a second Pooling layer labeled "P2" with a 2x2 window.
- This further reduces the spatial dimensions of the feature maps.

6. **Convolutional Layer (C3)**:
- The pooled feature maps are then passed through a third Convolutional layer labeled "C3" with 8 kernels of size 3x3.
- This layer produces 8 feature maps.

7. **Pooling Layer (P3)**:
- The output from the third Convolutional layer is passed through a third Pooling layer labeled "P3" with a 2x2 window.
- This further reduces the spatial dimensions.

8. **Dense Layers (D1 and D2)**:
- The final pooled feature maps are flattened and passed through two Dense layers.
- The first Dense layer (D1) has 256 neurons.
- The output from D1 is then passed through a second Dense layer (D2) with 128 neurons.

9. **Output Layer**:
- The final output from the Dense layer is passed to the Output layer, which produces the final prediction.

The diagram uses red arrows to show the flow of data through the network, and the feature maps are represented as a series of stacked, rectangular blocks. The dense layers are depicted as fully connected layers, with the final output layer shown as a single, vertical block.

FIGURE 3. Architecture of the proposed CNN.

Finally, we introduce the GZ1 image data and the perfor-
mance evaluation of the experiments (Section IV-D).

A. STANDARD CLASSIFICATION ALGORITHMS
The standard classiﬁcation algorithms selected for the com-
parative study were made according to their proven accurate
behaviour in many real-world applications [72]. Here we
use k-nearest neighbours, random forests, and support vector
machines. The basics of each algorithm and their conﬁgu-
rations are summarised in Table 4. We are not interested in
performing a ﬁne tuning of these algorithms, and we thus
employ the standard parameters.

• The k-nearest neighbours (kNN) [55] is a well-known
algorithm in supervised learning. The kNN algorithm
uses the whole training set as to classify new instances
via a similarity function, which is usually deﬁned by a
distance on the feature space. First, distances are com-
puted from the new example to the entire training set,
and then the k closest training instances are selected.
The ﬁnal label is chosen in accordance to the prevalent
class in the k-subset. Here, we use the Euclidean distance
and k = 3.

labels at the same time, thus leveraging the whole range of
knowledge featured by citizen science projects.
In the literature, pre-training has been used to alleviate the
scarcity of labelled examples and also save the higher com-
putational capacity and time required for the training of deep
architectures [63]. Hence, pre-trained networks, which have
been previously fully-trained using a more generic dataset,
are adapted (ﬁne-tuned) to the particular classiﬁcation prob-
lem at hand employing more speciﬁc data [61]. In practical
terms, this process generally entails the top dense layers of
the network that performs the classiﬁcation of the deep fea-
tures extracted by the convolutional layers. Taking advantage
of the set of weights previously learned in the pre-training
step, the dense layers are modiﬁed and trained to fulﬁl the
requirements of the targeted classiﬁcation problem [70].
Inspired by the problem of coarse supervision [71],
in which image labels followed a hierarchical structure (e.g.
coarse: big cat, ﬁner: leopard, cheetah, tiger), here we explore
a special case of pre-training by taking advantage of citizen
science data. First, we train the CNN using amateur-labelled
data, which is more abundant with respect to expert-labelled
data and considered less reliable [44]. Then, we complete the
ﬁne-tuning with examples labelled by experts, which we take
as ground truth for this problem.

• Random forests (RF) [73] is a classiﬁer based on the
decision tree algorithm [2]. It trains a number n of
decision tree classiﬁers and provides the majority class
among them as a result. Each sub-tree, referred as an
estimator, is trained using a sub-sample of the original
training set. Here we run the experiments with n = 100.
To measure the quality of a split, the Gini impurity is
used.

• Support vector machine (SVM) [74] is capable of learn-
ing a mapping from the input attributes to the set of
classes by means of a higher-dimensional feature space.
For this, a kernel function enables the computations of
the inner product between two feature vectors. After the

IV. EXPERIMENTAL SETUP
In this section, we explain the experimental setup deployed
for the comparative analysis of the classiﬁcation approaches
introduced above. First, we present the selected standard
classiﬁcation algorithms (Section IV-A) and the two chosen
FS methods (Section IV-B), along with their parameters. Both
standard classiﬁers and FS methods are applied to either
AEs or WND-CHARM feature sets. Then, we provide the
experimental details of the DL-based models, which include
both AEs, the proposed CNN, and ResNet (Section IV-C).

VOLUME 8, 2020
47237

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

TABLE 4. Parameters for the selected standard classifiers.

classiﬁcation, we introduce the ResNet [31] model as a com-
parison algorithm. ResNet is an architecture normally trained
to distinguish between multiple classes. Given that we focus
on a binary classiﬁcation problem, we use one of the lightest
versions of this network, the ResNet50 [70].
AE models, the proposed CNN and ResNet were trained
over 100 epochs with a batch size of 256 examples. CAE,
CNN and ResNet were optimised with stochastic gradient
descent [77], whereas DAE used the adadelta optimiser [78].

instances are mapped to the new feature space, the algo-
rithm looks for the optimal separating hyperplane, that
is, the one that maximises the distance to the different
classes clusters. Here we train the SVM using the linear
kernel.

B. FEATURE SELECTION METHODS
We consider two feature selection (FS) methods through
the comparative study, aiming to investigate the impact of
FS over the extracted feature sets in terms of classiﬁcation
runtime and accuracy:

• As mentioned in Section II-B.1, a FS method is proposed
as part of the WND-CHARM classiﬁer [35] and is based
on a feature ranking involving the use of Fisher discrim-
inant scores as feature weights [75]. First, the weights
are calculated for each feature following Equation (1):

Wf =
PN
c=1(Tf −Tf ,c)2

PN
c=1 σ 2
f ,c
·
N
N −1,
(1)

where Wf is the Fisher score of feature f , N is the
number of classes in the problem, Tf is the mean of
the values of feature f in the entire dataset, Tf ,c is the
mean of the values of feature f in the class c, and σ 2
f ,c
is the variance of feature f values across all examples
of class c. After the features are ranked according to
this weighting, the 35% holding a lower Fisher score are
rejected, as originally proposed in [35]. Therefore, this
Fisher FS method results in a ﬁxed number of features.

• The second FS method proposed is part of the so-called
embedded FS techniques, which highlights as a simple
yet fast strategy especially suitable for high-dimensional
data [76]. It implements a randomised decision tree
with 50 trees to choose the most relevant features. This
method also makes use of the labels, searching an opti-
mal subset of features in the combined space of features
and hypotheses. As such, the ﬁnal number of features
selected is variable, depending on the data sample
considered [77].

D. DATA AND EVALUATION OF EXPERIMENTS
The data used in the experiments is part of the collection of
galaxy images classiﬁed by the GZ1 project, which results
were published in the so-called GZ1 Table 2 (GZ1-T2) after
the project closure3 [19]. This dataset includes amateur clas-
siﬁcations for a total of 667,944 galaxy images. In addition,
we also hold expert classiﬁcations for a subset of the GZ1-
T2 data. This sample, referred from now on as GZ1 Expert
subset (GZ1-E), comprises an amount of 41,424 examples
that were classiﬁed as elliptical or spiral by a team of expert
astronomers [18].
The entire GZ1-T2 image dataset was primarily down-
loaded from the Sloan Digital Sky Server (SDSS) CAS
server.4 In order to establish a fair comparison, we follow the
original GZ1 project speciﬁcations [18], taking 423 × 423
pixels JPEG images centred in the galaxy. The image scale
is particular for each image and varies in accordance to
the formula 0.024Rp arcsec/pixel, where Rp is the Petrosian
radius for the galaxy, that is, a good estimator of its physical
size. However, we found that this automatic scaling tends to
leave the galaxy isolated in the centre of the image, with a
dominance of background pixels and/or other meaningless
artifacts around the target object. Therefore, in order to speed
up the image processing by both FE methods and the CNNs,
we simpliﬁed the images’ presentation and ended up con-
sidering two image sizes through the experiments, aiming to
study the inﬂuence of the image size and resolution in the
classiﬁcation performance: 128 × 128 (128x) and 64 × 64
(64x) pixels images. To accomplish this, we ﬁrst cropped
the original images in the GZ1-T2 dataset to their half size
(212 × 212 pixels) and converted them to TIFF format,
keeping the galaxy in the centre of the image. After this,
we compressed the resulting images to the two sizes referred
above.
For the GZ1-E, we used the expert classiﬁcations available
as image labels, which we take as ground truth for the prob-
lem. In contrast, the GZ1-T2 data provides the record of votes
for the options offered on the GZ1 web to project partici-
pants [18]. Hence, when using the whole data we assigned the
majority voted class among spiral and elliptical, considering
the original amateur classiﬁcations. However, this criterion
left 8,759 examples for which both scores coincided and thus
could not be labelled in this way. We opted to remove these

3The GZ1 results are available at http://data.galaxyzoo.org.
4http://cas.sdss.org

C. DL-BASED MODELS
In this paper we pursue a fair comparison between distinct
approaches for the classiﬁcation of galaxy images. Thus,
as stated above, the DL-based models used in the exper-
iments share a similar architecture and most parameters.
Nonetheless, to compare the proposed CNN against a
well-established deep neural network for end-to-end image

47238
VOLUME 8, 2020

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

TABLE 6. Number of extracted features in GZ1-E.

TABLE 5.
Description of the two data samples taken from the
GZ1-T2 dataset used in the experiments.

FIGURE 4. FE runtime in logarithmic scale for GZ1-E sample.

The image appears to be a depiction of a galaxy, likely taken by a space telescope. At the center, there is a bright, glowing core, which is the most luminous part of the image. Surrounding the core is a faint, diffuse halo that gradually fades into the surrounding space. The halo has a somewhat elliptical shape, with a concentration of stars and possibly gas and dust. The background is dark, filled with numerous small, faint points of light, representing distant stars. The overall appearance suggests a spiral or elliptical galaxy, with the central core being the focal point.

The image appears to be a blurred photograph of a bright light source, possibly a star or a planet, set against a dark background. The light source is central and emits a soft glow with a gradient of colors, transitioning from yellow at the center to green and then to a reddish hue. Surrounding the central light are faint, out-of-focus points of light, which could be other stars or celestial objects. The overall effect is a bokeh effect, where the background is dark and the light sources are diffused, creating a dreamy, ethereal appearance.

The image appears to be a blurred photograph of a celestial object, likely a star or galaxy, with a bright central region. The central area is glowing with a mix of yellow and green hues, suggesting it might be a star with a halo or a galaxy with a bright core. To the right, there is a smaller, more distinct blue-white light, possibly another star or a planet. The background is dark, typical of space imagery, with some indistinct green and brownish areas, which might be due to the camera's exposure or the presence of other celestial objects. The overall effect is a mix of colors and light, creating a vibrant yet blurry scene.

The image shows a galaxy with a bright central core, surrounded by a faint, diffuse halo. The core appears to be emitting a significant amount of light, creating a glowing effect. The halo has a somewhat elliptical shape, with a gradient that fades into the surrounding dark space. The overall appearance is reminiscent of a lenticular galaxy, which has a central core and a prominent halo or disk. The background is mostly dark, indicating that the galaxy is set against the vastness of space.

The image depicts a galaxy with a distinct spiral structure. The galaxy has a bright central core, from which the spiral arms extend outward. The arms are fainter as they move away from the center, creating a gradient of brightness. The overall shape is elongated, resembling a pinwheel. The background is dark, typical of a space image, with scattered stars visible around the galaxy. The colors range from yellow and white at the center to blue and purple in the outer regions of the spiral arms.

The image depicts a view of the night sky, featuring a prominent galaxy at the center. This galaxy has a bright core with a spiral structure, typical of a spiral galaxy. Surrounding the galaxy are several smaller stars and celestial objects, some of which appear to be stars or other galaxies. The background is dark, filled with numerous faint stars, creating a sense of depth and vastness in space. The colors range from blues and whites in the galaxy to yellows and reds in the nearby stars, adding to the visual interest of the scene.

The image appears to be a grayscale or black-and-white photograph. It features a central bright spot that gradually fades into a darker surrounding area. The bright spot is somewhat circular and has a soft, diffused glow, creating a halo effect around it. The rest of the image is predominantly dark, with the transition from the bright center to the dark periphery being smooth and gradual. The overall appearance is reminiscent of a light source or a lens flare effect.

The image appears to be a grayscale or black-and-white photograph. It features a central bright spot that gradually fades into a darker surrounding area. The bright spot is somewhat circular and has a soft, diffused edge, creating a halo effect around it. The rest of the image is predominantly dark, with no other discernible features or objects present. The overall appearance is simple and minimalistic, with a focus on the central light source.

The image appears to be a grayscale or black-and-white photograph. It features a central bright spot that gradually fades into a darker surrounding area. The bright spot is somewhat circular and has a soft, diffused edge, creating a halo effect. The overall appearance is reminiscent of a light source or a lens flare, with the brightness decreasing as it moves outward. The background is predominantly dark, which makes the central bright area stand out more prominently.

The image appears to be a grayscale or black-and-white photograph. It features a central, bright, circular area that gradually fades into a darker surrounding. The central bright spot is somewhat diffuse, creating a halo effect that diminishes in intensity as it moves outward. The overall appearance is reminiscent of a light source or lens flare, with the brightness concentrated in the middle and decreasing towards the edges. The background is predominantly dark, enhancing the contrast with the central light area.

The image appears to be a blurred or out-of-focus photograph. It features a faint, elongated, and somewhat oval-shaped light area in the center, surrounded by a dark background. The light area has a gradient, becoming lighter in the middle and gradually fading into the surrounding darkness. The overall appearance is soft and lacks sharp definition, making it difficult to discern specific details or objects within the image.

The image appears to be a grayscale or black-and-white photograph with a central, slightly blurred, bright spot. The background is predominantly dark, creating a stark contrast with the central light area. The bright spot has a soft, diffuse edge, suggesting it might be a light source or a reflection. The overall appearance is somewhat abstract, with no distinct shapes or objects discernible beyond the central light. The image has a dreamy or ethereal quality due to the lack of sharpness and the contrast between the light and dark areas.

The image appears to be a blurry, low-resolution photograph. It features a central, somewhat circular light source that is out of focus, creating a soft, glowing effect. The background is predominantly dark, with some indistinct shapes or areas of lighter coloration around the central light. The overall appearance is hazy, making it difficult to discern specific details or objects within the image. The edges of the image are framed by a dark border, which adds to the vignette effect, drawing attention to the central light.

The image appears to be a blurry, low-resolution photograph. The central focus is a light source, possibly a lamp or a window, which is out of focus, creating a soft, glowing effect. The surrounding area is dark, making it difficult to discern specific details. There are some indistinct shapes or objects on the right side, but they are not clearly visible due to the blur and low lighting. The overall impression is that of a dimly lit room with a central light source.

The image appears to be a blurry photograph with a dimly lit scene. There is a central light source that is out of focus, creating a soft, glowing effect. To the right, there is another, smaller light source that is also blurred. The background is dark, making the light sources stand out. The overall effect is hazy and indistinct, suggesting a low-light environment or a scene with motion blur.

The image appears to be a blurry and low-resolution photograph. It shows a light-colored, oval-shaped object against a dark background. The object is indistinct, making it difficult to identify its exact nature. The overall appearance is somewhat abstract due to the lack of clarity and focus.

The image appears to be a blurry and low-resolution photograph. It shows a horizontal, elongated object that is light-colored, possibly white or light gray, against a dark background. The object seems to be slightly curved or angled, and it's difficult to make out any specific details due to the blurriness. The overall scene is dimly lit, making it challenging to discern additional features or context.

The image appears to be a blurry photograph of a celestial scene, likely taken through a telescope or binoculars. The central focus is a bright, hazy object that could be a planet or a star cluster. Surrounding this central object are several smaller, fainter points of light, which might be other stars or celestial bodies. The overall image is dark, suggesting it was taken in a low-light environment, typical of astronomical photography. The lack of sharpness and clarity makes it difficult to identify specific details or objects within the image.

FIGURE 5. Sample of galaxy images reconstructed by the proposed AE
models. Top row presents the original 128x images from the
GZ1-T2 dataset. Middle and bottom rows show their reconstructions
performed by DAE and CAE architectures, respectively.

and to leverage the larger number of amateur-based classi-
ﬁcations with our proposed pre-training scheme for CNNs
(Subsection V-B). Finally, we analyse the results obtained
(Section V-C).

images from the GZ1-T2 data for consistency, then using
the remaining 659,185 images. We refer to this sample as
GZ1 Amateur subset (GZ1-A) from now on. The classes dis-
tribution of both GZ1-T2 data samples are shown in Table 5.
In all the experiments, we used a 5-fold cross-validation
scheme. For AEs, CNN and ResNet, the training set was
split into 70/30 for training and validation. In the classi-
ﬁcation with extracted feature sets, the classiﬁers training
were carried out consistently: same data partitions deﬁned
for the AEs training were used for the training and testing
of the classiﬁers, aiming to resemble real working conditions
for the classiﬁcation of unseen data.
Since the problem classes are balanced (Table 5), we drew
upon the classiﬁcation rate or accuracy (Acc) measure, which
accounts for the proportion of correct classiﬁcations with
respect to all classiﬁed examples [2]. We took as ﬁnal mea-
sure the average over the ﬁve test data partitions. We also
analysed the performance in terms of runtime, aiming to esti-
mate a runtime comparison between the different approaches
studied here. We examined the FE runtime, taken by both AE
models and WND-CHARM, and the classiﬁcation runtime
employed by the classiﬁers, including the CNN and ResNet.
For AEs, the runtime shown accounted for the training and
computation of features, whereas WND-CHARM directly
computed the features with no training. The classiﬁcation
runtime presented accounts for the training and classiﬁcation
stages, considering negligible the time taken by the FS phase,
when applied.
All experiments involving the classiﬁcation of the fea-
ture sets were carried out in a single node with an Intel(R)
Xeon(R) CPU E5-1650 v4 processor (12 cores) at 3.60GHz,
and 64 GB of RAM. For the training of the DL-based models,
we employed a NVIDIA Titan Xp GPU. In terms of software,
the Keras5 Python package was used for the AEs, CNN
and ResNet, and the Scikit-learn6 library for all experiments
involving either the training and classiﬁcation phases of the
standard classiﬁers introduced above. The WND-CHARM
implementation used here is freely available in Python lan-
guage at https://github.com/wnd-charm/wnd-charm.

V. RESULTS AND ANALYSIS
This section presents the experimental results. First, we carry
out a comparative study using the GZ1-E subset and consider-
ing both image sizes as well as colour and greyscale images
(Subsection V-A). Then, we focus on the GZ1-A subset to
investigate the scalability of the best classiﬁer from GZ1-E,

5https://keras.io/
6https://scikit-learn.org

A. GZ1-E: EXPERT SUBSET
Due to its reduced size, we ﬁrst used the GZ1-E sample
with expert labels, investigating the performances of both AE
models proposed as well as the inﬂuence of image size and
use of colour in classiﬁcation results. The CAE was tested
with colour and greyscale images, whereas the DAE was only
tested with greyscale images. For WND-CHARM, the two
feature sets available for colour and greyscale images were
computed. Thus, we obtained a distinct feature set for each
FE method and image colour/size conﬁguration.
The resultant number of features is indicated in Table 6,
and a visual comparison of the runtime spent by these
methods is presented in Figure 4. For visual illustration
of the behaviour of both AEs, Figure 5 plots the image

VOLUME 8, 2020
47239

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

FIGURE 7. Classification runtimes in logarithmic scale for 128x images of
the GZ1-E sample. Classifiers and FS methods are represented by colour
and intensity, respectively.

FIGURE 6. Classification runtimes in logarithmic scale for 64x images of
the GZ1-E sample. Classifiers and FS methods are represented by colour
and intensity, respectively.

FIGURE 8. Total classification runtime in logarithmic scale for both
strategies studied with the GZ1-E sample. For FE approaches,
the classification is performed with the RF classifier and embedded FS.
This runtime is subdivided in FE runtime and classification runtime.

reconstructions performed for a selection of images from
the GZ1-E sample. As it is shown, the DAE model disre-
gards colour channels and is less sensitive to the presence
of artefacts in the image and galaxy contours. Conversely,
the CAE model deﬁnes borders more accurately and partially
replicates colour in the images.
These feature sets were taken as input to the classiﬁers
selected for the study. In ﬁrst place, we carried out the classi-
ﬁcations with no FS, pursuing a ﬁrst comparison of the entire
AEs and WND-CHARM feature sets for greyscale and colour
images and both image sizes proposed. We then investigated
the application of the two FS methods proposed aiming to
speed up the classiﬁcation phase. Accuracy results for these
experiments are shown in Table 7 for both image sizes,
and comparative representations of the runtime are presented
in Figures 6 and 7 for 64x and 128x images, respectively.
These values correspond to the average of the classiﬁcation
runtime over the ﬁve data partitions.
In third place, we performed the classiﬁcation of the
GZ1-E images using the proposed CNN and the ResNet
model as a comparative algorithm. Here we also explored the
use of greyscale and colour images and the two image sizes
established for the study. These results are shown in Table 8.
Finally, we carried out a comparison among the total clas-
siﬁcation time of both strategies analysed using the GZ1-E
subset. As an estimation of this time for the approaches with
FE, we added the FE time of both AE models and WND-
CHARM to the classiﬁcation times obtained for each image
conﬁguration (Figures 4, 6 and 7). For the sake of simplicity,
here we selected the tandem Embedded FS plus RF classiﬁer
for both AE models and WND-CHARM feature sets classi-
ﬁcation, since this setting offered the best accuracy/runtime

trade-off in the experiments presented above (Table 7 and
Figures 6 and 7). This is represented in Figure 8.
For a visual illustration of the classiﬁcation problem inves-
tigated and how the different approaches and algorithms
work, Figure 9 displays a wide-ranging selection of images
from the GZ1-E subset. For simplicity, we restrict the illus-
tration to 128x colour images and select as well the combi-
nation Embedded FS plus RF classiﬁer for both AE models
and WND-CHARM feature sets classiﬁcation. We include
the result of DAE, CAE, and WND-CHARM FE methods,
and both CNN and ResNet classiﬁers. We also indicate the

47240
VOLUME 8, 2020

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

TABLE 7. Accuracy results for 64x and 128x images of GZ1-E sample, with no FS (top sector of the table), Fisher scores FS (middle sector) and embedded
FS (bottom sector).

TABLE 9. Accuracy results for GZ1-A sample. CAE and WND-CHARM
feature sets are classified with RF.

TABLE 8. Results of proposed CNN and ResNet for 64x and 128x images
of GZ1-E sample.

TABLE 10. Results of the CNN proposed and ResNet for GZ1-A sample.

amateur label for the object shown, considering the expert
classiﬁcation as ground truth.

Finally, we compared an estimation of the total classiﬁ-
cation time of both approaches analysed with the GZ1-A
sample. As we did with the GZ1-E subset, we added
the FE time to the classiﬁcation runtime for CAE and
WND-CHARM. These results are represented in Figure 11.

B. GZ1-A: AMATEUR SUBSET
Using the GZ1-A subset, the aim of this subsection is
two-fold. First, we used this bigger dataset to analyse the
scalability of the methods compared in the previous section
(Subsection V-B.1). Second, we exploited the huge number of
existing amateur-labelled galaxy images by pre-training both
CNN and ResNet models on this dataset (Subsection V-B.2).

1) SCALABILITY OF METHODS
After the ﬁrst set of experiments using the GZ1-E subset,
we extended the comparative study to the GZ1-A sample. For
this larger dataset, we only compared the features obtained
with the CAE and the WND-CHARM for 64x colour images.
We completed the classiﬁcation with RF algorithm, which
showed the best balance between runtime and accuracy in
the previous experiments with the GZ1-E sample. We also
examined the application of both FS methods proposed.
These results are shown in Table 9. The representation of
classiﬁcation runtime is presented in Figure 10, which also
includes the FE runtime for the CAE and WND-CHARM.
In line with the previous study of GZ1-E, we also carried
out the classiﬁcation with the proposed CNN and ResNet
on this sample. Results of accuracy and runtime are shown
in Table 10.

2) PRE-TRAINING WITH AMATEUR AND EXPERT LABELS
As we explain in Section III-C, citizen science projects enable
a novel methodology for the pre-training and ﬁne-tuning of
CNNs. By making use of amateur and expert classiﬁcations,
the network can be pre-trained employing amateur labels,
which are expected to be higher in number and coarser in
comparison with their expert counterparts. Then, the inclu-
sion of expert labels permits the ﬁne-tuning, occasionally re-
deﬁning the output (number of classes) of the network.
For this experiment, we considered 64x colour images.
We ﬁrst trained CNN and ResNet on GZ1-A using ama-
teur labels, and then re-trained the network (from previ-
ously learned weights for all layers) on GZ1-E with expert
labels carrying out the usual cross-validation established
for all experiments. Since the smaller GZ1-E sample is
included in GZ1-A, we removed the overlapping between
both samples. That is to say, the pre-train phase skipped the

VOLUME 8, 2020
47241

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

The image appears to be a photograph of a galaxy, likely taken through a telescope. The central part of the image is dominated by a bright, glowing core, which is typical of many galaxies. Surrounding the core is a diffuse, hazy region that might represent the galaxy's disk or halo. There are several smaller, distinct points of light around the central core, which could be stars or other celestial objects. The background is dark, indicating that the image was taken in space or a dark location to capture the faint light from these distant objects. The colors in the image range from yellow and white in the core to various hues in the surrounding areas, suggesting different temperatures and compositions of the materials in the galaxy.

The image appears to be a photograph of a celestial object, likely a star or galaxy, taken through a telescope. The central object is bright and has a glowing, somewhat diffuse appearance, suggesting it might be a star with a halo or a galaxy with a visible halo or nebula. The light is predominantly yellowish, with some reddish and greenish hues around the edges, indicating possible emission lines or dust.

In the background, the image is dark, typical of space photography, with a few faint points of light scattered around, which could be other stars or distant celestial objects. The overall appearance suggests a deep-space image, capturing the intricate details of a distant astronomical phenomenon.

The image appears to be a photograph of a galaxy, likely taken by a space telescope such as the Hubble Space Telescope. The central part of the image features a bright, glowing core, which is likely the galactic center. This core is surrounded by a faint, diffuse halo, suggesting the presence of a large amount of interstellar gas and dust.

Surrounding the central galaxy, there are several smaller, distinct points of light. These are likely other galaxies or stars, each varying in brightness and color. The colors range from white and blue to red, indicating different temperatures and compositions of the celestial objects. The background is predominantly dark, typical of deep-space images, with a few scattered stars visible.

Overall, the image captures a stunning view of the universe, highlighting the beauty and complexity of distant galaxies and their components.

The image shows a galaxy with a bright central core, surrounded by a faint, diffuse halo. The galaxy appears to have a spiral structure, with the core being the most luminous part. The surrounding space is dark, dotted with numerous small, faint stars. The overall appearance is typical of a spiral galaxy, with the central core possibly indicating a high concentration of stars and other celestial objects. The image captures the galaxy against the backdrop of the night sky, highlighting its luminosity and structure.

The image appears to be a photograph of a galaxy, likely taken by a space telescope. The central part of the galaxy is bright, with a yellowish core, which is probably the galactic nucleus. Surrounding the core is a diffuse, glowing halo, indicating the presence of a large amount of gas and dust. There are also several smaller, distinct stars visible around the galaxy, appearing as bright points against the dark background of space. The overall appearance suggests that this is an image of a spiral galaxy, with the central core and halo being prominent features.

The image appears to be a blurred photograph of a galaxy. The galaxy has a distinct elongated shape, with a bright central region that gradually fades into a more diffuse tail. The colors in the galaxy range from yellow to green, with a hint of red near the center. The background is dark, typical of space imagery, with some out-of-focus light spots that might be other celestial objects. The overall effect is a soft, glowing appearance against the dark backdrop.

The image appears to be a photograph of a galaxy taken by a space telescope, likely the Hubble Space Telescope. The central part of the galaxy is bright and luminous, with a glowing core that emits light. Surrounding the core is a halo of colors, including green, red, and yellow, which may represent different gases and dust in the galaxy. The outer regions of the galaxy are darker, with a textured appearance that could be due to the presence of stars and interstellar material. There are also a few small, bright points near the top of the image, which might be other stars or celestial objects. The overall image has a high level of detail, showcasing the intricate structures within the galaxy.

The image shows a small, faint object in the night sky, which appears to be a comet. The comet has a bright central core with a gradient of colors, transitioning from a yellowish hue at the center to reddish and bluish tones towards the edges. The tail of the comet is elongated and slightly blurred, suggesting it is moving or dispersing. The background is dark, typical of a night sky, with no other visible stars or celestial objects.

The image shows a galaxy with a distinct spiral structure. The galaxy has a bright central core, from which the spiral arms extend outward. The arms are faint and appear to be made of stars and interstellar gas. The background is dark, filled with small, scattered stars, typical of a deep-space photograph. The overall appearance is characteristic of a barred spiral galaxy, with the central bar structure contributing to the spiral pattern.

The image shows a galaxy with a bright, central core surrounded by a faint, diffuse halo. The core appears to be a yellowish-white color, indicating a high concentration of stars and possibly a supermassive black hole at the center. The halo has a slightly reddish tint, suggesting the presence of older stars or interstellar dust. The galaxy is set against a dark background filled with numerous small, faint stars, typical of a deep-space photograph. The overall appearance is reminiscent of a spiral galaxy, with the central core and halo forming a distinct, elongated shape.

The image depicts a galaxy with a bright central core, surrounded by a diffuse, glowing halo. The core appears to be a luminous point, possibly a supermassive black hole or a dense cluster of stars. The halo extends outward, creating a nebulous, cloud-like structure that gradually fades into the surrounding space. The background is dark, filled with scattered stars, enhancing the prominence of the galaxy. The overall appearance is reminiscent of a lenticular or spiral galaxy, with the central light source being the focal point.

The image appears to be a photograph of a galaxy. The galaxy has a distinct spiral shape, with a bright central core that is likely the galactic bulge. The spiral arms extend outward from the core, displaying a mix of colors, including yellow and blue, which may indicate different temperatures and compositions of the stars and interstellar material. The background is dark, typical of deep-space images, with some faint stars or other celestial objects visible. The overall appearance suggests a typical spiral galaxy, possibly similar to the Milky Way.

The image appears to be a photograph of a galaxy, likely taken by a space telescope such as the Hubble Space Telescope. The galaxy is centrally located and features a bright core, which is the most luminous part of the image. Surrounding the core, there is a halo of light with various colors, including green, blue, and red, indicating different types of stars and interstellar material. The outer regions of the galaxy are darker, suggesting a dense concentration of stars and possibly a spiral structure. The background is mostly dark, typical of deep-space images, with a few faint stars visible. The overall appearance suggests a vibrant and active galactic environment.

The image appears to be a photograph of a galaxy, likely taken through a telescope. The galaxy is centrally located and features a bright core with a glowing, yellowish hue. Surrounding the core is a faint, diffuse halo that gradually fades into the surrounding darkness. The background is mostly dark, indicating the vastness of space. The galaxy's structure suggests it might be a spiral galaxy, with the core and halo possibly representing the central bulge and spiral arms, though the arms are not distinctly visible in this image. The overall appearance is typical of a galaxy captured in infrared or visible light, highlighting its central region and extending outwards into space.

The image appears to be a close-up view of a small, bright, and somewhat blurry object against a dark background. The object has a glowing, yellowish center with a reddish hue around it, suggesting it might be a celestial body or a light source. The surrounding area is dark and indistinct, with some faint, possibly text-like patterns or noise, which makes the central object stand out prominently. The overall appearance is reminiscent of a star or a planet in space, with the surrounding darkness enhancing its visibility.

The image appears to be a photograph of a galaxy, likely taken through a telescope. The central part of the image is dominated by a bright, glowing core, which is typical of many galaxies. This core is surrounded by a diffuse, hazy halo, suggesting the presence of a large amount of interstellar dust and gas. The outer regions of the galaxy are less bright and more diffuse, blending into the dark background of space.

In the lower left corner, there is a smaller, distinct point of light, which could be another star or a smaller celestial object. The overall color palette of the image includes shades of white, yellow, and brown, with the background being predominantly dark, indicating the vastness of space. The image has a somewhat blurry or out-of-focus quality, which is common in astronomical photography due to the distance and the light-gathering capabilities of telescopes.

FIGURE 9. Sample of 128x colour images from the GZ1-E subset showing a wide-range of image qualities and
difficulty. Two top rows are classified by experts as elliptical and two bottom rows as spiral. For each of the images,
we indicate which approaches misclassify (red) or correctly predict the label (blue), considering expert
classifications as ground truth.

examples in GZ1-A later used in the ﬁne-tuning with GZ1-E.
We refer to this data sample as GZ1-A*, which consisted
of 617,986 examples. In contrast with usual pre-training
approaches [61]–[63], here the number of classes does not
change and therefore we kept the dense part of both networks
for the ﬁne-tuning. Results for this experiment are shown
in Table 11.

• Among the FE approaches compared, AEs have demon-
strated to perform the extraction of features in a shorter
amount of time, as it is shown in Figures 4 and 10. How-
ever, the classiﬁcation with the WND-CHARM feature
set provided better accuracy compared to AE features
across both GZ1 data samples (Table 7 for GZ1-E and 9
for GZ1-A). In broad terms, RF generally outperformed
the kNN and SVM algorithms and the use of 64x or
128x images did not make a big difference. Nonetheless,
the presence of colour provided better accuracy with
respect to greyscale images. This was accomplished at
the cost of higher runtime as well, in particular for

C. ANALYSIS OF RESULTS
An examination of the tables and charts presented above
allowed us to conclude the following remarks after the
experiments:

47242
VOLUME 8, 2020

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

classiﬁcation time with the GZ1-E dataset is comparable
to the classiﬁcation using the proposed CNN (Figure 8).
This conﬁrms the potential utility of AEs in the classi-
ﬁcation of large amounts of image data, given that the
AE’s training would be completed only once.

FIGURE 10. FE and classification runtimes in logarithmic scale for GZ1-A
sample.

• Both analysed CNNs provided the best performance in
comparison with the three FE approaches studied in
terms of accuracy/runtime balance for both data sam-
ples used in the study (Tables 8 and 10). Nonethe-
less, the WND-CHARM Colour feature set was able to
obtain comparable accuracy in the GZ1-E subset when
the classiﬁcation was made using the SVM algorithm
(Table 7). The difference was enlarged with the GZ1-A
sample (Table 9), showing that CNNs coped better with
the learning from larger amounts of data that probably
contain more noise in the labels, and also revealing
that amateur labels tended to degrade the classiﬁcation
accuracy.

FIGURE 11. Total classification runtime in logarithmic scale for both
strategies studied with the GZ1-A sample. For FE approaches,
the classification is performed with the RF classifier and embedded FS.
This runtime is subdivided in FE runtime and classification runtime.

TABLE 11. Results for the proposed CNN and ResNet, implementing the
pre-training and fine-tuning with the GZ1-A* and GZ1-E subsets,
respectively.

• ResNet generally outperformed the proposed CNN
model in terms of accuracy in the experiments with
GZ1-E with expert labels. However, this was achieved
at the cost of a much higher runtime that grew up to
seven times for greyscale images (Table 8). In contrast,
the improvement was marginal in the classiﬁcation of
GZ1-A with amateur labels, where a huge increase of
the runtime did not provide a much better accuracy
(Table 10). These experiments reveal that deeper archi-
tectures do not always translate in much more improved
results, and that the selection of the model can be critical
for an optimal DL classiﬁcation approach in terms of
time.

the WND-CHARM Colour feature extractor, which has
proved to be the best feature set in terms of classiﬁcation
accuracy.

• The two FS methods proposed did not have a big impact
on the classiﬁcation accuracy. However, they consid-
erably diminished the classiﬁcation runtime, especially
for the WND-CHARM feature sets, as it is presented
in Figures 6 and 7. The most promising results were
obtained for the embedded FS method, probably due to
the dynamic nature of this approach that selects a vari-
able number of features. Conversely, the method based
on Fisher scores always ﬁlters a ﬁxed number of fea-
tures, thus providing a runtime reduction that remained
steady.

• The proposed pre-training and ﬁne-tuning scheme,
using both amateur and expert labels, showed a promis-
ing result as a way of leveraging all the potential of cit-
izen science projects (Table 11). Here the improvement
in accuracy (comparing with the previous experiment
with GZ1-E only, Table 8), was greater for the proposed
CNN, indicating that the addition of more layers in the
network did not provide any substantial improvement
with coarser labels. However, the pre-training phase
considerably enlarged the total runtime for both CNNs.
This experiment conﬁrmed the adequacy of considering
expert and amateur labels to feed the learning of a ML
approach, which specially applies in complex classiﬁca-
tion problems such as the one studied in this paper. For
example, images 9d, 9e, 9l and 9n demonstrate that even
the best approaches compared in the study are prone to
fail (if we consider experts’ judgements as ground truth),
and an integrated use of all knowledge about the problem
(e.g. expert classiﬁcations, additional astronomical data,
citizen science results) can be crucial.

• Although the WND-CHARM Colour feature set yielded
better classiﬁcation accuracy, both AE models proposed
here have proved to greatly accelerate the FE process
(Figures 4 and 10), which could be decisive for the
classiﬁcation of big volumes of data. Among the AE
models proposed, the CAE provided the best results in
terms of accuracy with respect to the DAE, also enabling
the use of colour images. For this architecture, the global

VI. CONCLUSION
In this paper, we have presented a comparative study about
the performance of two different strategies for the automated
classiﬁcation of galaxy images, either classifying a feature

VOLUME 8, 2020
47243

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

set obtained from the image or with convolutional neural net-
works. Through a set of experiments, we have compared the
state-of-the-art feature extractor, the WND-CHARM, with
the suitability of autoencoders for feature extraction of galaxy
images. We have then compared these results with the end-
to-end classiﬁcation provided by two models of convolu-
tional neural networks under the same experimental setting.
We have explored the impact of the image size and the pres-
ence or not of the colour channels in the classiﬁcation results,
also studying the effect of two distinct feature selection
methods. The experiments have been run using two different
samples from the Galaxy Zoo 1 image dataset, also studying
the scalability of both approaches to larger data and the inﬂu-
ence of amateur and expert classiﬁcations in the classiﬁcation
accuracy. In addition, we have introduced a novel approach
based on pre-training and ﬁne-tuning of convolutional neural
networks that have proven to take advantage of both label sets
available for this problem.
The results allow us to conclude that convolutional neu-
ral networks offer the best trade-off between runtime and
accuracy although the addition of a big depth and complex-
ity in the network does not always provide a signiﬁcant
improvement in their prediction capability, depending on the
classiﬁcation problem at hand. Also, autoencoders represent
a promising alternative for the classiﬁcation of these images
with feature extraction. This is a consequence of their ability
to separate the feature extraction and learning processes,
which could eventually be beneﬁcial when the amount of data
to be classiﬁed expands. Finally, it has been shown that very
promising results may eventually come from the learning of
both amateur and expert label sets that citizen science projects
offer. Following the work presented here, we plan to enhance
the learning phase with the consideration of unlabelled data in
conjunction with different levels of conﬁdence in the images
labelling.

ACKNOWLEDGMENT
The authors gratefully acknowledge the support of NVIDIA
Corporation with the donation of the Titan Xp GPU used for
this research.

REFERENCES

[1] S. B. Kotsiantis, ‘‘Supervised machine learning: A review of classiﬁcation
techniques,’’ Informatica, vol. 31, no. 1, pp. 249–268, 2007.
[2] I. H. Witten and E. Frank, Data Mining: Practical Machine Learning Tools
and Techniques, 2nd ed. San Francisco, CA, USA: Morgan Kaufmann,
2005.
[3] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classiﬁcation, 2nd ed.
New York, NY, USA: Wiley, 2001.
[4] D. Peralta, I. Triguero, S. García, Y. Saeys, J. M. Benitez, and F. Herrera,
‘‘On the use of convolutional neural networks for robust classiﬁcation
of multiple ﬁngerprint captures,’’ Int. J. Intell. Syst., vol. 33, no. 1,
pp. 213–230, Nov. 2018.
[5] D. K. Jain, P. Shamsolmoali, and P. Sehdev, ‘‘Extended deep neural net-
work for facial emotion recognition,’’ Pattern Recognit. Lett., vol. 120,
pp. 69–74, Apr. 2019.
[6] D. Lu, M. Heisler, S. Lee, G. W. Ding, E. Navajas, M. V. Sarunic, and
M. F. Beg, ‘‘Deep-learning based multiclass retinal ﬂuid segmentation and
detection in optical coherence tomography images using a fully convolu-
tional neural network,’’ Med. Image Anal., vol. 54, pp. 100–110, May 2019.

[7] M. Umehara, H. S. Stein, D. Guevarra, P. F. Newhouse, D. A. Boyd, and
J. M. Gregoire, ‘‘Analyzing machine learning models to accelerate genera-
tion of fundamental materials insights,’’ npj Comput. Mater., vol. 5, no. 1,
pp. 1–9, Mar. 2019.
[8] J. Tyson, ‘‘Optical synoptic telescopes: New science frontiers,’’ in Proc.
The Int. Soc. for Opt. Eng., vol. 7733, no. 1, 2010, Art. no. 773303.
[9] K. D. Borne, ‘‘Astroinformatics: Data-oriented astronomy research and
education,’’ Earth Sci. Informat., vol. 3, nos. 1–2, pp. 5–17, May 2010.
[10] S. Ackermann, K. Schawinski, C. Zhang, A. K. Weigel, and M. D. Turp,
‘‘Using transfer learning to detect galaxy mergers,’’ Monthly Notices Roy.
Astronomical Soc., vol. 479, no. 1, pp. 415–425, May 2018.
[11] R. E. González, R. P. Muñoz, and C. A. Hernández, ‘‘Galaxy detection
and identiﬁcation using deep learning and data augmentation,’’ Astron.
Comput., vol. 25, pp. 103–109, Oct. 2018.
[12] C.
E.
Petrillo,
C.
Tortora,
S.
Chatterjee,
G.
Vernardos,
L. V. E. Koopmans, G. V. Kleijn, N. R. Napolitano, G. Covone,
L. S. Kelvin, and A. M. Hopkins, ‘‘Testing convolutional neural networks
for ﬁnding strong gravitational lenses in KiDS,’’ Monthly Notices Roy.
Astronomical Soc., vol. 482, no. 1, pp. 807–820, 2019.
[13] E. Hubble, ‘‘Extra-galactic nebulae,’’ Astrophys. J., vol. 64, pp. 321–373,
Dec. 1926.
[14] A. Sandage,‘‘The classiﬁcation of galaxies: Early history and ongoing
developments,’’ Annu. Rev. Astron. Astrophys., vol. 43, no. 1, pp. 581–624,
Sep. 2005.
[15] K. Borne, Virtual Observatories, Data Mining, and Astroinformatics.
Dordrecht, The Netherlands: Springer, 2013.
[16] J. P. Cohn, ‘‘Citizen science: Can volunteers do real research?’’ BioScience,
vol. 58, no. 3, pp. 192–197, Mar. 2008.
[17] R. Simpson, K. Page, and D. De Roure, ‘‘Zooniverse: Observing the
world’s largest citizen science platform,’’ in Proc. Int. Conf. World Wide
Web, 2014, pp. 1049–1054.
[18] C. Lintott, K. Schawinski, A. Slosar, K. Land, S. Bamford, D. Thomas,
M. Raddick, R. Nichol, A. Szalay, D. Andreescu, P. Murray, and
J. Vandenberg, ‘‘Galaxy Zoo: Morphologies derived from visual inspection
of galaxies from the sloan digital sky survey,’’ Monthly Notices Roy.
Astronomical Soc., vol. 389, no. 3, pp. 1179–1189, 2008.
[19] C. Lintott, K. Schawinski, S. Bamford, A. Slosar, K. Land, D. Thomas,
E. Edmondson, K. Masters, R. Nichol, M. Raddick, A. Szalay,
D. Andreescu, P. Murray, and J. Vandenberg, ‘‘Galaxy Zoo 1: Data release
of morphological classiﬁcations for nearly 900 000 galaxies,’’ Monthly
Notices Roy. Astronomical Soc., vol. 410, no. 1, pp. 166–178, 2011.
[20] Z. Ivezić, S. Kahn, J. Tyson, B. Abel, E. Acosta, R. Allsman, D. Alonso,
Y. Alsayyad, S. Anderson, J. Andrew, and, ‘‘LSST: From science drivers to
reference design and anticipated data products,’’ Astrophysical J., vol. 873,
no. 2, pp. 1–44, 2019.
[21] M. Banerji, O. Lahav, C. Lintott, F. Abdalla, K. Schawinski, S. Bamford,
D. Andreescu, P. Murray, M. Raddick, A. Slosar, A. Szalay, D. Thomas,
and J. Vandenberg, ‘‘Galaxy zoo: Reproducing galaxy morphologies via
machine learning,’’ Monthly Notices Roy. Astronomical Soc., vol. 406,
no. 1, pp. 342–353, 2010.
[22] L. Shamir, ‘‘Automatic detection of peculiar galaxies in large datasets of
galaxy images,’’ J. Comput. Sci., vol. 3, no. 3, pp. 181–189, May 2012.
[23] S. Dieleman, K. W. Willett, and J. Dambre, ‘‘Rotation-invariant con-
volutional neural networks for galaxy morphology prediction,’’ Monthly
Notices Roy. Astronomical Soc., vol. 450, no. 2, pp. 1441–1459, Apr. 2015.
[24] X.-P. Zhu, J.-M. Dai, C.-J. Bian, Y. Chen, S. Chen, and C. Hu, ‘‘Galaxy
morphology classiﬁcation with deep convolutional neural networks,’’
Astrophys. Space Sci., vol. 364, no. 4, pp. 1–15, Apr. 2019.
[25] M. Nixon and A. Aguado, Feature Extraction & Image Processing, 2nd ed.
Orlando, FL, USA: Academic, 2008.
[26] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge,
MA, USA: MIT Press, 2016.
[27] Y. Bengio, ‘‘Learning deep architectures for AI,’’ Found. Trends Mach.
Learn., vol. 2, no. 1, pp. 1–127, 2009.
[28] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
G. Wang, J. Cai, and T. Chen, ‘‘Recent advances in convolutional neural
networks,’’ Pattern Recognit., vol. 77, pp. 354–377, May 2018.
[29] W. Rawat and Z. Wang, ‘‘Deep convolutional neural networks for image
classiﬁcation: A comprehensive review,’’ Neural Comput., vol. 29, no. 9,
pp. 2352–2449, Sep. 2017.
[30] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for
large-scale image recognition,’’ in Proc. Int. Conf. Learn. Represent., 2015.

47244
VOLUME 8, 2020

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

[52] J. De La Calleja and O. Fuentes, ‘‘Machine learning and image analysis for
morphological galaxy classiﬁcation,’’ Monthly Notices Roy. Astronomical
Soc., vol. 349, no. 1, pp. 87–93, Mar. 2004.
[53] M. Huertas-Company, D. Rouan, L. Tasca, G. Soucail, and O. Le Fèvre,
‘‘A robust morphological classiﬁcation of high-redshift galaxies using
support vector machines on seeing limited images: I. method description,’’
Astron. Astrophys., vol. 478, no. 3, pp. 971–980, Nov. 2008.
[54] L. Shamir, N. Orlov, D. M. Eckley, T. Macura, J. Johnston, and
I. G. Goldberg, ‘‘Wndchrm—An open source utility for biological image
analysis,’’ Source Code for Biol. Med., vol. 3, no. 1, pp. 1–13, Jul. 2008.
[55] T. Cover and P. Hart, ‘‘Nearest neighbor pattern classiﬁcation,’’ IEEE
Trans. Inf. Theory, vol. IT-13, no. 1, pp. 21–27, Jan. 1967.
[56] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, ‘‘Going deeper with convolutions,’’
in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015,
pp. 1–9.
[57] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, ‘‘Aggregated residual
transformations for deep neural networks,’’ in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 5987–5995.
[58] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
M. Tan, X. Wang, W. Liu, and B. Xiao, ‘‘Deep high-resolution represen-
tation learning for visual recognition,’’ 2019, arXiv:1908.07919. [Online].
Available: http://arxiv.org/abs/1908.07919
[59] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘ImageNet:
A large-scale hierarchical image database,’’ in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit., Jun. 2009, pp. 248–255.
[60] D. Yu and M. Seltzer, ‘‘Improved bottleneck features using pretrained deep
neural networks,’’ in Proc. Annu. Conf. Int. Speech Commun. Assoc., 2011,
pp. 237–240.
[61] D. Marmanis, M. Datcu, T. Esch, and U. Stilla, ‘‘Deep learning Earth
observation classiﬁcation using ImageNet pretrained networks,’’ IEEE
Geosci. Remote Sens. Lett., vol. 13, no. 1, pp. 105–109, Jan. 2016.
[62] U. K. Lopes and J. F. Valiati, ‘‘Pre-trained convolutional neural networks as
feature extractors for tuberculosis detection,’’ Comput. Biol. Med., vol. 89,
pp. 135–143, Oct. 2017.
[63] B. Kieffer, M. Babaie, S. Kalra, and H. R. Tizhoosh, ‘‘Convolutional neural
networks for histopathology image classiﬁcation: Training vs. using pre-
trained networks,’’ in Proc. 7th Int. Conf. Image Process. Theory, Tools
Appl. (IPTA), Nov. 2018, pp. 1–6.
[64] M. Huertas-Company, R. Gravet, G. Cabrera-Vives, P. G. Pérez-González,
J. S. Kartaltepe, G. Barro, M. Bernardi, S. Mei, F. Shankar, P. Dimauro, E.
F. Bell, D. Kocevski, D. C. Koo, S. M. Faber, and D. H. Mcintosh, ‘‘A cat-
alog of visual-like morphologies in the 5 CANDELS ﬁelds using deep
learning,’’ Astrophys. J. Suppl. Ser., vol. 221, no. 1, pp. 1–23, Oct. 2015.
[65] M. Ranzato, Y. Boureau, Y. LeCun, and S. Chopra, ‘‘A uniﬁed energy-
based framework for unsupervised learning,’’ J. Mach. Learn. Res., vol. 2,
pp. 371–379, 2007.
[66] H. Robbins and S. Monro, ‘‘A stochastic approximation method,’’ Ann.
Math. Statist., vol. 22, no. 3, pp. 400–407, 1951.
[67] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‘‘Learning rep-
resentations by back-propagating errors,’’ Nature, vol. 323, no. 6088,
pp. 533–536, Oct. 1986.
[68] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, ‘‘Gradient-based learn-
ing applied to document recognition,’’ Proc. IEEE, vol. 86, no. 11,
pp. 2278–2324, 1998.
[69] V. Nair and G. Hinton, ‘‘Rectiﬁed linear units improve restricted
Boltzmann machines,’’ in Proc. 27th Int. Conf. Mach. Learn., 2010,
pp. 807–814.
[70] M. Mahdianpari, B. Salehi, M. Rezaee, F. Mohammadimanesh, and
Y. Zhang, ‘‘Very deep convolutional neural networks for complex land
cover mapping using multispectral remote sensing imagery,’’ Remote
Sens., vol. 10, no. 7, p. 1119, Jul. 2018.
[71] F. Taherkhani, H. Kazemi, A. Dabouei, J. Dawson, and N. Nasrabadi,
‘‘A weakly supervised ﬁne label classiﬁer enhanced by coarse supervi-
sion,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019,
pp. 6459–6468.
[72] X. Wu, V. Kumar, Q. Ross, J. Ghosh, Q. Yang, H. Motoda, G. McLachlan,
A. Ng, B. Liu, P. Yu, Z.-H. Zhou, M. Steinbach, D. Hand, and D. Steinberg,
‘‘Top 10 algorithms in data mining,’’ Knowl. Inf. Syst., vol. 14, no. 1,
pp. 1–37, 2008.
[73] L. Breiman, ‘‘Random forests,’’ Mach. Learn., vol. 45, no. 1, pp. 5–32,
2001.
[74] C. Burges, ‘‘A tutorial on support vector machines for pattern recognition,’’
Data Mining Knowl. Discovery, vol. 2, no. 2, pp. 121–167, 1998.

[31] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image
recognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),
Jun. 2016, pp. 770–778.
[32] D. Charte, F. Charte, S. García, M. J. del Jesus, and F. Herrera, ‘‘A practical
tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models,
software and guidelines,’’ Inf. Fusion, vol. 44, pp. 78–96, Nov. 2018.
[33] F. J. Pulgar, F. Charte, A. J. Rivera, and M. J. del Jesus, ‘‘AEkNN:
An AutoEncoder kNN-based classiﬁer with built-in dimensionality reduc-
tion,’’ Int. J. Comput. Intell. Syst., vol. 12, no. 1, pp. 436–452, 2018.
[34] N. Ball and R. Brunner, ‘‘Data mining and machine learning in astronomy,’’
Int. J. Mod. Phys. D, vol. 19, no. 7, pp. 1049–1106, 2010.
[35] N. Orlov, L. Shamir, T. Macura, J. Johnston, D. M. Eckley, and
I. G. Goldberg, ‘‘WND-CHARM: Multi-purpose image classiﬁcation
using compound image transforms,’’ Pattern Recognit. Lett., vol. 29,
no. 11, pp. 1684–1693, Aug. 2008.
[36] L. Shamir, A. Holincheck, and J. Wallin, ‘‘Automatic quantitative mor-
phological analysis of interacting galaxies,’’ Astron. Comput., vol. 2,
pp. 67–73, Aug. 2013.
[37] E. Kuminski and L. Shamir, ‘‘A computer-generated visual morphology
catalog of ~ 3,000,000 SDSS galaxies,’’ Astrophys. J. Suppl. Ser., vol. 223,
no. 2, pp. 1–10, 2016.
[38] S. van den Bergh, ‘‘A new classiﬁcation system for galaxies,’’ Astrophys.
J., vol. 206, pp. 883–887, Jun. 1976.
[39] R. E. Hart, S. P. Bamford, W. C. Keel, S. J. Kruk, K. L. Masters,
B. D. Simmons, and R. J. Smethurst, ‘‘Galaxy Zoo: Constraining the origin
of spiral arms,’’ Monthly Notices Roy. Astronomical Soc., vol. 478, no. 1,
pp. 932–949, May 2018.
[40] C. Watson, K.-V. Tran, A. Tomczak, L. Alcorn, I. V. Salazar, A. Gupta,
I. Momcheva, C. Papovich, P. V. Dokkum, G. Brammer, J. Lotz, and
C. N. A. Willmer, ‘‘Galaxy merger fractions in two clusters at z ∼2 using
the hubble space telescope,’’ Astrophys. J., vol. 874, no. 1, p. 63, Mar. 2019.
[41] R. M. G. Delgado, E. Pérez, R. C. Fernandes, R. García-Benito,
R. L. Fernández, N. V. Asari, C. Cortijo-Ferrero, A. L. de Amorim,
E. A. D. Lacerda, S. F. Sánchez, M. D. Lehnert, and C. J. Walcher,
‘‘Spatially-resolved star formation histories of CALIFA galaxies: Implica-
tions for galaxy formation,’’ Astron. Astrophys., vol. 607, no. 128, pp. 1–21,
2017.
[42] R. Bonney, J. L. Shirk, T. B. Phillips, A. Wiggins, H. L. Ballard,
A. J. Miller-Rushing, and J. K. Parrish, ‘‘Next steps for citizen science,’’
Science, vol. 343, no. 6178, pp. 1436–1437, Mar. 2014.
[43] L. Trouille, C. J. Lintott, and L. F. Fortson, ‘‘Citizen science frontiers:
Efﬁciency, engagement, and serendipitous discovery with human–machine
systems,’’ Proc. Nat. Acad. Sci. USA, vol. 116, no. 6, pp. 1902–1909,
Feb. 2019.
[44] M. Jiménez, I. Triguero, and R. John, ‘‘Handling uncertainty in citizen sci-
ence data: Towards an improved amateur-based large-scale classiﬁcation,’’
Inf. Sci., vol. 479, pp. 301–320, Apr. 2019.
[45] K. Willett, C. Lintott, S. Bamford, K. Masters, B. Simmons, K. Casteels,
E. Edmondson, L. Fortson, S. Kaviraj, W. Keel, T. Melvin, R. Nichol,
M. J. Raddick, K. Schawinski, R. Simpson, R. Skibba, A. Smith, and
D. Thomas, ‘‘Galaxy Zoo 2: Detailed morphological classiﬁcations for
304,122 galaxies from the Sloan Digital Sky Survey,’’ Monthly Notices
Roy. Astronomical Soc., vol. 435, no. 4, pp. 2835–2860, 2013.
[46] K. Willett, M. A. Galloway, S. P. Bamford, C. J. Lintott, K. L. Masters,
C. Scarlata, B. D. Simmons, M. Beck, C. N. Cardamone, E. Cheung, and
E. M. Edmondson, ‘‘Galaxy Zoo: Morphological classiﬁcations for 120
000 galaxies in HST legacy imaging,’’ Monthly Notices Roy. Astronomical
Soc., vol. 464, no. 4, pp. 4176–4203, 2017.
[47] M. Thonnat and M. Berthod, ‘‘Automatic classiﬁcation of galaxies into
morphological types,’’ in Proc. Int. Conf. Pattern Recognit., vol. 2, 1984,
pp. 844–846.
[48] O. Lahav, A. Naim, R. J. Buta, H. G. Corwin, G. de Vaucouleurs,
A. Dressler, J. P. Huchra, S. van den Bergh, S. Raychaudhury, L. Sodre,
and M. C. Storrie-Lombardi, ‘‘Galaxies, human eyes, and artiﬁcial neural
networks,’’ Science, vol. 267, no. 5199, pp. 859–862, Feb. 1995.
[49] I. A. Basheer and M. Hajmeer, ‘‘Artiﬁcial neural networks: Fundamentals,
computing, design, and application,’’ J. Microbiol. Methods, vol. 43, no. 1,
pp. 3–31, Dec. 2000.
[50] C. J. Conselice, M. A. Bershady, and A. Jangren, ‘‘The asymmetry of
galaxies: Physical morphology for nearby and high-redshift galaxies,’’
Astrophys. J., vol. 529, no. 2, pp. 886–910, Feb. 2000.
[51] J. M. Lotz, J. Primack, and P. Madau, ‘‘A new nonparametric approach
to galaxy morphological classiﬁcation,’’ Astronomical J., vol. 128, no. 1,
pp. 163–182, Jul. 2004.

VOLUME 8, 2020
47245

The image shows the logo of "IEEE Access." The text is split into two parts: "IEEE" is written in bold, dark blue capital letters, while "Access" is written in light blue lowercase letters. The "Access" part is slightly smaller in size compared to "IEEE." The registered trademark symbol (®) is placed at the upper right corner of the word "Access." The overall design is clean and modern, with a simple color scheme of blue and white.

M. Jiménez et al.: Galaxy Image Classification Based on Citizen Science Data: Comparative Study

The image is a black and white photograph of a man. He has a receding hairline and a full beard. He is smiling slightly and has a friendly expression. He is wearing a suit jacket over a collared shirt. The background appears to be indoors, with a window or some vertical blinds visible behind him. The overall tone of the image is formal and professional.

[75] C. Bishop, Pattern Recognition and Machine Learning. Berlin, Germany:
Springer, 2006.
[76] Y. Saeys, I. Inza, and P. Larranaga, ‘‘A review of feature selection tech-
niques in bioinformatics,’’ Bioinformatics, vol. 23, no. 19, pp. 2507–2517,
Aug. 2007.
[77] R. G. J. Wijnhoven and P. H. N. de With, ‘‘Fast training of object detection
using stochastic gradient descent,’’ in Proc. 20th Int. Conf. Pattern Recog-
nit., Aug. 2010, pp. 424–427.
[78] S. Akila Agnes and J. Anitha, ‘‘Analyzing the effect of optimization
strategies in deep convolutional neural network,’’ in Nature Inspired Opti-
mization Techniques for Image Processing Applications (Intelligent Sys-
tems Reference Library), vol. 150. Cham, Switzerland: Springer, 2019,
pp. 235–253.

ROBERT JOHN (Senior Member, IEEE) received
the bachelor’s degree (Hons.) in mathematics,
the M.Sc. degree in statistics, and the Ph.D. degree
in type-2 fuzzy logic. His initial career was as a
mathematician in various roles for industry and
commerce as an AI Consultant. He joined De
Montfort University, in 1989, and the University
of Nottingham, in 2013. He is currently a Professor
of operational research and computer science, and
the Head of the Computational Optimisation and
Learning (COL) Laboratory, School of Computer Science, University of
Nottingham. He is also an elected member of the EPSRC College. In the
ﬁeld of type-2 fuzzy logic, his work is widely recognized by the international
fuzzy logic community as leading in the aspects of theoretical foundations
and practical applications. His work has produced many fundamental new
results that have opened the ﬁeld to new research, enabling a broadening
of scope and application. He has signiﬁcant funding for his research from
a variety of sources. He has over 8500 citations on Google Scholar with an
H-index of 39. He is also a Fellow of the British Computer Society.

MANUEL JIMÉNEZ received the M.Sc. degree in
physics from the University of Granada, Granada,
Spain, in 2016. He is currently pursuing the Ph.D.
degree with the Computational Optimisation and
Learning (COL) Laboratory, School of Computer
Science, University of Nottingham. His research
interests include the application of machine learn-
ing techniques to the classiﬁcation of astronomical
images under conditions of uncertainty.

The image is a black and white photograph of a person. The individual has short, slightly wavy hair and is looking directly at the camera with a neutral expression. The person has a fair complexion and is wearing a dark-colored shirt. The background is plain and light, providing a clear contrast to the subject. The image appears to be a headshot, focusing on the person's face and upper shoulders.

The image is a black and white photograph of a man. He has short, dark hair and a beard. He is smiling, showing his teeth. The man is wearing a plain white shirt. The background is plain and light-colored, providing a clear contrast to his face. The image appears to be a headshot, likely used for identification or professional purposes.

The image is a black and white photograph of a person smiling at the camera. The person is wearing large, dark sunglasses with a classic rectangular shape. They have long, straight hair that falls over their shoulders. The person is dressed in a dark jacket with a zipper and a light-colored top underneath.

In the background, there is a traditional architectural structure, possibly a gate or entrance, with a tiled roof. There are trees and some other people visible in the distance, suggesting the setting might be a public or historical area. The overall mood of the image is cheerful and relaxed.

ISAAC TRIGUERO (Member, IEEE) received the
M.Sc. and Ph.D. degrees in computer science
from the University of Granada, Granada, Spain,
in 2009 and 2014, respectively. He has been an
Assistant Professor of data science with the Uni-
versity of Nottingham, since June 2016. His work
is mostly concerned with the research of novel
methodologies for big data analytics. He has pub-
lished more than 70 international publications in
the ﬁelds of big data, machine learning, and opti-
mization (H-index = 24 and more than 2300 citations on Google Scholar).
He has acted as the Program Co-Chair of the IEEE Conference on Smart
Data, in 2016, the IEEE Conference on Big Data Science and Engineering,
in 2017, and the IEEE International Congress on Big Data, in 2018. He is
currently leading the Knowledge Transfer Partnership Project funded by the
Innovative U.K. and the energy provider E.ON, that investigates smart meter-
ing data. He is also the Section Editor-in-Chief of the Machine Learning and
Knowledge Extraction journal and an Associate Editor of the Big Data and
Cognitive Computing journal and IEEE ACCESS journal.

MERCEDES TORRES
TORRES received the
M.Sc. degree in digital signal and image pro-
cessing from Cranﬁeld University, in 2010, and
the Ph.D. degree in computer science from the
University of Nottingham, in 2014. She has been
an Assistant Professor of computer science with
the University of Nottingham, since 2015, where
she is a member of the Computer Vision Lab-
oratory and the Horizon Research Institute. She
works in the development and application of new
machine learning techniques, particularly deep learning, for skewed or small
datasets. She is currently leading the Knowledge Transfer Partnership funded
by the Innovate U.K., and Netacea investigating ﬁne-grained web trafﬁc
classiﬁcation.

47246
VOLUME 8, 2020