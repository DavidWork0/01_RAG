{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88934dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.14.1)\n",
      "Requirement already satisfied: haystack-experimental in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (0.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (4.23.0)\n",
      "Requirement already satisfied: lazy-imports in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (0.4.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (10.7.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (2.2.5)\n",
      "Requirement already satisfied: openai>=1.56.1 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (1.78.0)\n",
      "Requirement already satisfied: posthog!=3.12.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (4.0.1)\n",
      "Requirement already satisfied: pydantic in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (2.11.4)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (9.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-ai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.56.1->haystack-ai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->haystack-ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->haystack-ai) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->haystack-ai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->haystack-ai) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->haystack-ai) (0.4.6)\n",
      "Requirement already satisfied: filetype in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from haystack-experimental->haystack-ai) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->haystack-ai) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema->haystack-ai) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema->haystack-ai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema->haystack-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema->haystack-ai) (0.24.0)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.25.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\t440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytesseract) (10.4.0)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install haystack-ai\n",
    "!pip install pymupdf  \n",
    "!pip install pillow  \n",
    "!pip install pandas  \n",
    "!pip install pytesseract  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e21883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from typing import List, Dict, Any, Optional\n",
    "from haystack import Document, component\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0cc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class PDFToJSONProcessor:\n",
    "    \"\"\"\n",
    "    A Haystack component that processes PDFs and extracts text, images, and tables\n",
    "    into a structured JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, extract_images: bool = True, extract_tables: bool = True):\n",
    "        self.extract_images = extract_images\n",
    "        self.extract_tables = extract_tables\n",
    "    \n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, sources: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process PDF files and return structured JSON data.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of PDF file paths\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing processed documents\n",
    "        \"\"\"\n",
    "        processed_documents = []\n",
    "        \n",
    "        for pdf_path in sources:\n",
    "            try:\n",
    "                json_data = self._process_pdf(pdf_path)\n",
    "                doc = Document(content=json.dumps(json_data, indent=2))\n",
    "                doc.meta[\"source\"] = pdf_path\n",
    "                doc.meta[\"content_type\"] = \"structured_json\"\n",
    "                processed_documents.append(doc)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"documents\": processed_documents}\n",
    "    \n",
    "    def _process_pdf(self, pdf_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract all content from PDF and structure it as JSON.\"\"\"\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        result = {\n",
    "            \"document_info\": {\n",
    "                \"filename\": pdf_path.split(\"/\")[-1],\n",
    "                \"total_pages\": len(pdf_document),\n",
    "                \"metadata\": pdf_document.metadata\n",
    "            },\n",
    "            \"pages\": []\n",
    "        }\n",
    "        \n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            page_data = {\n",
    "                \"page_number\": page_num + 1,\n",
    "                \"text_content\": self._extract_text(page),\n",
    "                \"images\": self._extract_images(page) if self.extract_images else [],\n",
    "                \"tables\": self._extract_tables(page) if self.extract_tables else []\n",
    "            }\n",
    "            result[\"pages\"].append(page_data)\n",
    "        \n",
    "        pdf_document.close()\n",
    "        return result\n",
    "    \n",
    "    def _extract_text(self, page) -> Dict[str, Any]:\n",
    "        \"\"\"Extract text content with formatting information.\"\"\"\n",
    "        text_dict = page.get_text(\"dict\")\n",
    "        \n",
    "        text_content = {\n",
    "            \"raw_text\": page.get_text(),\n",
    "            \"structured_text\": [],\n",
    "            \"fonts_used\": set()\n",
    "        }\n",
    "        \n",
    "        for block in text_dict[\"blocks\"]:\n",
    "            if \"lines\" in block:  # Text block\n",
    "                block_data = {\n",
    "                    \"bbox\": block[\"bbox\"],\n",
    "                    \"lines\": []\n",
    "                }\n",
    "                \n",
    "                for line in block[\"lines\"]:\n",
    "                    line_data = {\n",
    "                        \"bbox\": line[\"bbox\"],\n",
    "                        \"spans\": []\n",
    "                    }\n",
    "                    \n",
    "                    for span in line[\"spans\"]:\n",
    "                        span_data = {\n",
    "                            \"text\": span[\"text\"],\n",
    "                            \"font\": span[\"font\"],\n",
    "                            \"size\": span[\"size\"],\n",
    "                            \"flags\": span[\"flags\"],\n",
    "                            \"bbox\": span[\"bbox\"]\n",
    "                        }\n",
    "                        line_data[\"spans\"].append(span_data)\n",
    "                        text_content[\"fonts_used\"].add(span[\"font\"])\n",
    "                    \n",
    "                    block_data[\"lines\"].append(line_data)\n",
    "                text_content[\"structured_text\"].append(block_data)\n",
    "        \n",
    "        text_content[\"fonts_used\"] = list(text_content[\"fonts_used\"])\n",
    "        return text_content\n",
    "    \n",
    "    def _extract_images(self, page) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract images from the page with proper error handling.\"\"\"\n",
    "        images = []\n",
    "        \n",
    "        try:\n",
    "            # Get the full image list with all metadata\n",
    "            image_list = page.get_images(full=True)\n",
    "            \n",
    "            for img_index, img_item in enumerate(image_list):\n",
    "                try:\n",
    "                    # Extract xref from the full image item\n",
    "                    xref = img_item[0]  # First element is always xref\n",
    "                    \n",
    "                    # Check if this is a valid image reference\n",
    "                    if xref <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract image using document's extract_image method\n",
    "                    base_image = page.parent.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    \n",
    "                    # Get image position using the full image item\n",
    "                    try:\n",
    "                        # For images with item[-1] != 0, we need special handling\n",
    "                        if img_item[-1] != 0:\n",
    "                            # Try to get bbox using XObject list as fallback\n",
    "                            xobject_list = page.parent.get_page_xobjects(page.number)\n",
    "                            img_rect = None\n",
    "                            for xobj in xobject_list:\n",
    "                                if xobj[0] == xref:\n",
    "                                    img_rect = fitz.Rect(xobj[1])\n",
    "                                    break\n",
    "                            \n",
    "                            if img_rect is None:\n",
    "                                # Use page dimensions as fallback\n",
    "                                img_rect = page.rect\n",
    "                        else:\n",
    "                            # Standard bbox extraction\n",
    "                            img_rect = page.get_image_bbox(img_item)\n",
    "                            \n",
    "                    except Exception:\n",
    "                        # Fallback: try to find image position using get_image_rects\n",
    "                        try:\n",
    "                            rects = page.get_image_rects(img_item)\n",
    "                            img_rect = rects[0] if rects else page.rect\n",
    "                        except Exception:\n",
    "                            img_rect = page.rect  # Use full page as last resort\n",
    "                    \n",
    "                    # Convert to base64\n",
    "                    img_base64 = base64.b64encode(image_bytes).decode()\n",
    "                    \n",
    "                    image_info = {\n",
    "                        \"image_index\": img_index,\n",
    "                        \"xref\": xref,\n",
    "                        \"bbox\": list(img_rect),\n",
    "                        \"width\": base_image.get(\"width\", 0),\n",
    "                        \"height\": base_image.get(\"height\", 0),\n",
    "                        \"colorspace\": base_image.get(\"colorspace\", \"unknown\"),\n",
    "                        \"ext\": base_image.get(\"ext\", \"png\"),\n",
    "                        \"base64_data\": img_base64,\n",
    "                        \"smask\": img_item[1] if len(img_item) > 1 else 0  # Mask reference\n",
    "                    }\n",
    "                    images.append(image_info)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting image {img_index}: {str(e)}\")\n",
    "                    # Add placeholder for failed extraction\n",
    "                    images.append({\n",
    "                        \"image_index\": img_index,\n",
    "                        \"xref\": img_item[0] if img_item else -1,\n",
    "                        \"error\": str(e),\n",
    "                        \"bbox\": [0, 0, 0, 0],\n",
    "                        \"extraction_failed\": True\n",
    "                    })\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting image list: {str(e)}\")\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def _extract_tables(self, page) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract tables from the page.\"\"\"\n",
    "        tables = []\n",
    "        \n",
    "        try:\n",
    "            # Find tables using PyMuPDF's table detection\n",
    "            tabs = page.find_tables()\n",
    "            \n",
    "            for tab_index, tab in enumerate(tabs):\n",
    "                table_data = {\n",
    "                    \"table_index\": tab_index,\n",
    "                    \"bbox\": list(tab.bbox),\n",
    "                    \"rows\": [],\n",
    "                    \"column_count\": 0,\n",
    "                    \"row_count\": 0\n",
    "                }\n",
    "                \n",
    "                # Extract table content\n",
    "                table_content = tab.extract()\n",
    "                if table_content:\n",
    "                    table_data[\"rows\"] = table_content\n",
    "                    table_data[\"row_count\"] = len(table_content)\n",
    "                    table_data[\"column_count\"] = len(table_content[0]) if table_content else 0\n",
    "                \n",
    "                tables.append(table_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting tables: {str(e)}\")\n",
    "        \n",
    "        return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b969bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Implementation\n",
    "@component\n",
    "class JSONStructureEnhancer:\n",
    "    \"\"\"\n",
    "    Enhances the extracted JSON with additional processing and cleaning.\n",
    "    \"\"\"\n",
    "    \n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, documents: List[Document]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhance and clean the extracted JSON data.\n",
    "        \"\"\"\n",
    "        enhanced_documents = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            try:\n",
    "                json_data = json.loads(doc.content)\n",
    "                enhanced_data = self._enhance_structure(json_data)\n",
    "                \n",
    "                enhanced_doc = Document(content=json.dumps(enhanced_data, indent=2))\n",
    "                enhanced_doc.meta = doc.meta.copy()\n",
    "                enhanced_doc.meta[\"processing_stage\"] = \"enhanced\"\n",
    "                enhanced_documents.append(enhanced_doc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error enhancing document: {str(e)}\")\n",
    "                enhanced_documents.append(doc)  # Return original if enhancement fails\n",
    "        \n",
    "        return {\"documents\": enhanced_documents}\n",
    "    \n",
    "    def _enhance_structure(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Add summary statistics and content analysis.\"\"\"\n",
    "        enhanced = data.copy()\n",
    "        \n",
    "        # Add document-level statistics\n",
    "        enhanced[\"document_statistics\"] = {\n",
    "            \"total_text_length\": sum(len(page[\"text_content\"][\"raw_text\"]) \n",
    "                                   for page in data[\"pages\"]),\n",
    "            \"total_images\": sum(len(page[\"images\"]) for page in data[\"pages\"]),\n",
    "            \"total_tables\": sum(len(page[\"tables\"]) for page in data[\"pages\"]),\n",
    "            \"pages_with_images\": sum(1 for page in data[\"pages\"] if page[\"images\"]),\n",
    "            \"pages_with_tables\": sum(1 for page in data[\"pages\"] if page[\"tables\"])\n",
    "        }\n",
    "        \n",
    "        # Add content type classification for each page\n",
    "        for page in enhanced[\"pages\"]:\n",
    "            page[\"content_analysis\"] = {\n",
    "                \"has_text\": bool(page[\"text_content\"][\"raw_text\"].strip()),\n",
    "                \"has_images\": bool(page[\"images\"]),\n",
    "                \"has_tables\": bool(page[\"tables\"]),\n",
    "                \"text_length\": len(page[\"text_content\"][\"raw_text\"]),\n",
    "                \"dominant_content\": self._classify_page_content(page)\n",
    "            }\n",
    "        \n",
    "        return enhanced\n",
    "    \n",
    "    def _classify_page_content(self, page: Dict[str, Any]) -> str:\n",
    "        \"\"\"Classify the dominant content type of a page.\"\"\"\n",
    "        text_length = len(page[\"text_content\"][\"raw_text\"])\n",
    "        image_count = len(page[\"images\"])\n",
    "        table_count = len(page[\"tables\"])\n",
    "        \n",
    "        if table_count > 0 and text_length < 500:\n",
    "            return \"table_heavy\"\n",
    "        elif image_count > 0 and text_length < 200:\n",
    "            return \"image_heavy\"\n",
    "        elif text_length > 1000:\n",
    "            return \"text_heavy\"\n",
    "        elif image_count > 0 and table_count > 0:\n",
    "            return \"mixed_content\"\n",
    "        else:\n",
    "            return \"minimal_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "227bcc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e26cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:\\Users\\T440\\Documents\\GitHub\\00_AudiRAG\\data\\raw\\Galaxy_Image_Classification_Based_on_Citizen_Science_Data_A_Comparative_Study autoencoder.pdf\n"
     ]
    }
   ],
   "source": [
    "def create_pdf_processing_pipeline():\n",
    "    \"\"\"Create a complete PDF processing pipeline.\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    pdf_processor = PDFToJSONProcessor(\n",
    "        extract_images=True,\n",
    "        extract_tables=True\n",
    "    )\n",
    "    json_enhancer = JSONStructureEnhancer()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(\"pdf_processor\", pdf_processor)\n",
    "    pipeline.add_component(\"json_enhancer\", json_enhancer)\n",
    "    \n",
    "    # Connect components\n",
    "    pipeline.connect(\"pdf_processor.documents\", \"json_enhancer.documents\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Usage\n",
    "def process_pdfs(pdf_files: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process PDF files and return structured JSON data.\"\"\"\n",
    "    \n",
    "    pipeline = create_pdf_processing_pipeline()\n",
    "    \n",
    "    # Run the pipeline\n",
    "    result = pipeline.run({\n",
    "        \"pdf_processor\": {\"sources\": pdf_files}\n",
    "    })\n",
    "    \n",
    "    # Extract and parse results\n",
    "    processed_data = []\n",
    "    for doc in result[\"json_enhancer\"][\"documents\"]:\n",
    "        json_data = json.loads(doc.content)\n",
    "        processed_data.append(json_data)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_files = [r\"C:\\Users\\T440\\Documents\\GitHub\\00_AudiRAG\\data\\raw\\Galaxy_Image_Classification_Based_on_Citizen_Science_Data_A_Comparative_Study autoencoder.pdf\"]\n",
    "    results = process_pdfs(pdf_files)\n",
    "    \n",
    "    # Save results\n",
    "    for i, result in enumerate(results):\n",
    "        with open(f\"output_{i}.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        print(f\"Processed {result['document_info']['filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb3c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open JSON and reconstruct base64 images\n",
    "def reconstruct_images_from_json(json_file: str) -> List[Image.Image]:\n",
    "    \"\"\"Reconstruct images from a JSON file containing base64 encoded images.\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    images = []\n",
    "    for page in data.get(\"pages\", []):\n",
    "        for img in page.get(\"images\", []):\n",
    "            if \"base64_data\" in img:\n",
    "                img_data = base64.b64decode(img[\"base64_data\"])\n",
    "                image = Image.open(io.BytesIO(img_data))\n",
    "                images.append(image)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb722dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save images to disk\n",
    "def save_images_to_disk(images: List[Image.Image], output_dir: str):\n",
    "    \"\"\"Save a list of PIL Images to the specified directory.\"\"\"\n",
    "    for i, img in enumerate(images):\n",
    "        img_path = f\"{output_dir}/image_{i}.png\"\n",
    "        img.save(img_path)\n",
    "        print(f\"Saved image to {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967bcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = reconstruct_images_from_json(\"output_0.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe31212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image to output_images/image_0.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image to output_images/image_1.png\n",
      "Saved image to output_images/image_2.png\n",
      "Saved image to output_images/image_3.png\n",
      "Saved image to output_images/image_4.png\n",
      "Saved image to output_images/image_5.png\n",
      "Saved image to output_images/image_6.png\n",
      "Saved image to output_images/image_7.png\n",
      "Saved image to output_images/image_8.png\n",
      "Saved image to output_images/image_9.png\n",
      "Saved image to output_images/image_10.png\n",
      "Saved image to output_images/image_11.png\n",
      "Saved image to output_images/image_12.png\n",
      "Saved image to output_images/image_13.png\n",
      "Saved image to output_images/image_14.png\n",
      "Saved image to output_images/image_15.png\n",
      "Saved image to output_images/image_16.png\n",
      "Saved image to output_images/image_17.png\n",
      "Saved image to output_images/image_18.png\n",
      "Saved image to output_images/image_19.png\n",
      "Saved image to output_images/image_20.png\n",
      "Saved image to output_images/image_21.png\n",
      "Saved image to output_images/image_22.png\n",
      "Saved image to output_images/image_23.png\n",
      "Saved image to output_images/image_24.png\n",
      "Saved image to output_images/image_25.png\n",
      "Saved image to output_images/image_26.png\n",
      "Saved image to output_images/image_27.png\n",
      "Saved image to output_images/image_28.png\n",
      "Saved image to output_images/image_29.png\n",
      "Saved image to output_images/image_30.png\n",
      "Saved image to output_images/image_31.png\n",
      "Saved image to output_images/image_32.png\n",
      "Saved image to output_images/image_33.png\n",
      "Saved image to output_images/image_34.png\n",
      "Saved image to output_images/image_35.png\n",
      "Saved image to output_images/image_36.png\n",
      "Saved image to output_images/image_37.png\n",
      "Saved image to output_images/image_38.png\n",
      "Saved image to output_images/image_39.png\n",
      "Saved image to output_images/image_40.png\n",
      "Saved image to output_images/image_41.png\n",
      "Saved image to output_images/image_42.png\n",
      "Saved image to output_images/image_43.png\n",
      "Saved image to output_images/image_44.png\n",
      "Saved image to output_images/image_45.png\n",
      "Saved image to output_images/image_46.png\n",
      "Saved image to output_images/image_47.png\n",
      "Saved image to output_images/image_48.png\n",
      "Saved image to output_images/image_49.png\n",
      "Saved image to output_images/image_50.png\n",
      "Saved image to output_images/image_51.png\n",
      "Saved image to output_images/image_52.png\n",
      "Saved image to output_images/image_53.png\n",
      "Saved image to output_images/image_54.png\n",
      "Saved image to output_images/image_55.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "if not os.path.exists(\"output_images\"):\n",
    "    os.makedirs(\"output_images\")\n",
    "# Now save the images again\n",
    "save_images_to_disk(images, \"output_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d09d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Captions:\n",
      "Image 0: the logo for the ieee access program\n",
      "Image 1: the logo for the ieee access\n",
      "Image 2: the logo for the ieee access\n",
      "Image 3: the logo for the ieee access\n",
      "Image 4: a diagram showing the different stages of the ecdg\n",
      "Image 5: the logo for the ieee access\n",
      "Image 6: a diagram of the two different types of the electromagnetic spectrum\n",
      "Image 7: the diagram shows the different layers of the milky\n",
      "Image 8: the logo for the ieee access\n",
      "Image 9: the logo for the ieee access\n",
      "Image 10: the logo for the ieee access\n",
      "Image 11: a black and white image of a light\n",
      "Image 12: a black and white photo of a man in a suit\n",
      "Image 13: a black and white image of a light in the dark\n",
      "Image 14: a black square object with a white background\n",
      "Image 15: a black and white image of a light\n",
      "Image 16: the andromus galaxy, a large galaxy in the constellation\n",
      "Image 17: a very large white object in the dark sky\n",
      "Image 18: a black and white image of a light in the dark\n",
      "Image 19: the galaxies in the constellation cluster\n",
      "Image 20: a black and white image of a light in the dark\n",
      "Image 21: a black and white image of a light in the dark\n",
      "Image 22: a black square with a white background\n",
      "Image 23: a square black object\n",
      "Image 24: a black and white photo of a man\n",
      "Image 25: a black and white photo of a light\n",
      "Image 26: a bright light shining in the dark sky\n",
      "Image 27: a black hole in the sky\n",
      "Image 28: a bright yellow disk in the dark sky\n",
      "Image 29: the logo for the ieee access\n",
      "Image 30: the logo for the ieee access\n",
      "Image 31: the logo for the ieee access\n",
      "Image 32: the andromus galaxy\n",
      "Image 33: a black hole in the sky\n",
      "Image 34: the galaxies and their companion stars\n",
      "Image 35: a bright yellow light in the dark sky\n",
      "Image 36: the galaxies in the constellation\n",
      "Image 37: the sun in the sky\n",
      "Image 38: a black hole in the sky\n",
      "Image 39: a bright light in the dark sky\n",
      "Image 40: a bright light in the dark sky\n",
      "Image 41: a black hole in the sky\n",
      "Image 42: the galaxies, with a bright white disk in the center\n",
      "Image 43: a bright yellow light in the dark sky\n",
      "Image 44: a very dark night sky\n",
      "Image 45: a bright yellow light in the dark sky\n",
      "Image 46: a very dark, dark, and bright galaxy\n",
      "Image 47: a black hole in the ground\n",
      "Image 48: the logo for the ieee access\n",
      "Image 49: the logo for the ieee access\n",
      "Image 50: the logo for the ieee access\n",
      "Image 51: the logo for the ieee access\n",
      "Image 52: a man in a black shirt is looking at the camera\n",
      "Image 53: a woman wearing sunglasses and smiling for the camera\n",
      "Image 54: a man in a white shirt smiling\n",
      "Image 55: a man in a suit and tie standing in front of a building\n"
     ]
    }
   ],
   "source": [
    "#USE THIS FOR QUICK GENERATION\n",
    "\n",
    "#iterate over images and generate captions\n",
    "captions = []\n",
    "def generate_caption(image: Image.Image) -> str:\n",
    "    \"\"\"Generate a caption for a given image using the BLIP model.\"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "for image in images:\n",
    "    caption = generate_caption(image)\n",
    "    captions.append(caption)\n",
    "    \n",
    "print(\"Generated Captions:\")\n",
    "for i, caption in enumerate(captions):\n",
    "    print(f\"Image {i}: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ca36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Detailed Descriptions:\n",
      "Image 0: the ieee access logo\n",
      "Image 1: the ieee access logo\n",
      "Image 2: the ieee access logo\n",
      "Image 3: the ieee access logo\n",
      "Image 4: a diagram showing the different types of the leds\n",
      "Image 5: the ieee access logo\n",
      "Image 6: a diagram showing the different layers of an image\n",
      "Image 7: a diagram showing the different types of galaxies\n",
      "Image 8: the ieee access logo\n",
      "Image 9: the ieee access logo\n",
      "Image 10: the ieee access logo\n",
      "Image 11: a black and white image of the moon\n",
      "Image 12: an image of a black and white photo\n",
      "Image 13: a black and white image of a light in the dark\n",
      "Image 14: a black and white photo of a large object\n",
      "Image 15: a black and white image of a light in the dark\n",
      "Image 16: an image of a galaxy in the dark sky\n",
      "Image 17: a black hole in the middle of a black hole\n",
      "Image 18: a black and white image of the moon\n",
      "Image 19: an image of a galaxy in the sky\n",
      "Image 20: a black and white image of a light in the dark\n",
      "Image 21: a black and white image of a light in the dark\n",
      "Image 22: an image of a black square in the dark\n",
      "Image 23: an image of a light in the dark\n",
      "Image 24: a black and white photo of a woman\n",
      "Image 25: a black and white photo of an empty room\n",
      "Image 26: a black hole in the center of a galaxy\n",
      "Image 27: a black hole in the sky\n",
      "Image 28: an image of a black hole in the sky\n",
      "Image 29: the ieee access logo\n",
      "Image 30: the ieee access logo\n",
      "Image 31: the ieee access logo\n",
      "Image 32: the andromus galaxy in the constellation\n",
      "Image 33: an image of a black hole in the sky\n",
      "Image 34: ngc and ngc in the sky\n",
      "Image 35: an image of the sun in the dark sky\n",
      "Image 36: the galaxies in the sky\n",
      "Image 37: an image of a black hole in the sky\n",
      "Image 38: a black hole in the sky\n",
      "Image 39: an image of a black hole in the sky\n",
      "Image 40: an image of a black hole in the sky\n",
      "Image 41: an image of a black hole in the sky\n",
      "Image 42: an image of a galaxy in the dark sky\n",
      "Image 43: an image of a black hole in the sky\n",
      "Image 44: an image of a light in the dark sky\n",
      "Image 45: an image of a black hole in the sky\n",
      "Image 46: an image of a black hole in the sky\n",
      "Image 47: an image of a bright light in the dark\n",
      "Image 48: the ieee access logo\n",
      "Image 49: the ieee access logo\n",
      "Image 50: the ieee access logo\n",
      "Image 51: the ieee access logo\n",
      "Image 52: a black and white photo of a man\n",
      "Image 53: a woman wearing sunglasses and smiling for the camera\n",
      "Image 54: a black and white photo of a man with a beard\n",
      "Image 55: a black and white photo of a man in a suit\n"
     ]
    }
   ],
   "source": [
    "#This is much better with this detailed description generation.!!!!!!!\n",
    "#Generate a detailed image description\n",
    "def generate_detailed_description(image: Image.Image) -> str:\n",
    "    \"\"\"Generate a detailed description for a given image using the BLIP model.\"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=500, num_beams=5, early_stopping=True)\n",
    "    description = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return description\n",
    "\n",
    "# Generate detailed descriptions for all images\n",
    "detailed_descriptions = []\n",
    "for image in images:\n",
    "    description = generate_detailed_description(image)\n",
    "    detailed_descriptions.append(description)\n",
    "print(\"Generated Detailed Descriptions:\")\n",
    "for i, description in enumerate(detailed_descriptions):\n",
    "    print(f\"Image {i}: {description}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\", ProgressBar=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0745221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Detailed Descriptions:\n",
      "Image 0: an image of a blue and white logo with the words ieee access\n",
      "Image 1: this is an image of a blue and white logo with the words life access\n",
      "Image 2: this is an image of a blue and white logo with the words life access\n",
      "Image 3: this is an image of a blue and white logo with the words life access\n",
      "Image 4: an image of a diagram showing the different layers of a sound system\n",
      "Image 5: this is an image of a blue and white logo with the words life access\n",
      "Image 6: an image of a diagram of the process of producing a new product\n",
      "Image 7: an image of a diagram of the structure of a galaxy\n",
      "Image 8: this is an image of a blue and white logo with the words life access\n",
      "Image 9: this is an image of a blue and white logo with the words life access\n",
      "Image 10: this is an image of a blue and white logo with the words life access\n",
      "Image 11: a black and white photo of a bright light in the dark\n",
      "Image 12: an image of a blurry image of a black hole in the sky\n",
      "Image 13: a black and white photo of a bright light in the dark\n",
      "Image 14: a close up of a black hole in the dark sky\n",
      "Image 15: a black and white photo of a light in the dark\n",
      "Image 16: this is an image of a spiral galaxy in the night sky\n",
      "Image 17: this is an image of a spiral shaped object in the night sky\n",
      "Image 18: a black and white photo of a bright light in the dark\n",
      "Image 19: this is an image of a spiral galaxy in the night sky\n",
      "Image 20: a black and white photo of a bright light in the dark\n",
      "Image 21: a black and white photo of a spiral shaped object in the dark\n",
      "Image 22: there is a blurry image of a light in the dark\n",
      "Image 23: this is an image of a black square with a light in the middle\n",
      "Image 24: there is a blurry image of a person ' s face in the dark\n",
      "Image 25: this is an image of a blurry image of a light in the dark\n",
      "Image 26: this is an image of a bright light in the dark night sky\n",
      "Image 27: this is an image of a bright light in the dark sky\n",
      "Image 28: this is an image of a bright light in the dark night sky\n",
      "Image 29: this is an image of a blue and white logo with the words life access\n",
      "Image 30: this is an image of a blue and white logo with the words life access\n",
      "Image 31: this is an image of a blue and white logo with the words life access\n",
      "Image 32: this is an image of a spiral galaxy in the night sky\n",
      "Image 33: this is an image of a bright object in the dark sky\n",
      "Image 34: this is an image of a spiral galaxy in the night sky\n",
      "Image 35: this is an image of a planetary object in the night sky\n",
      "Image 36: this is an image of a spiral galaxy in the night sky\n",
      "Image 37: this is an image of a planetary object in the night sky\n",
      "Image 38: this is an image of a spiral shaped object in the sky\n",
      "Image 39: this is an image of a bright light in the dark night sky\n",
      "Image 40: this is an image of a group of stars in the night sky\n",
      "Image 41: this is an image of a spiral shaped object in the night sky\n",
      "Image 42: this is an image of a spiral shaped object in the night sky\n",
      "Image 43: this is an image of an image of a star in the sky\n",
      "Image 44: there is a bright light in the middle of a dark forest\n",
      "Image 45: this is an image of a planetary object in the night sky\n",
      "Image 46: this is an image of a bright light in the dark sky\n",
      "Image 47: this is an image of a blurry image of a bright light\n",
      "Image 48: this is an image of a blue and white logo with the words life access\n",
      "Image 49: this is an image of a blue and white logo with the words life access\n",
      "Image 50: this is an image of a blue and white logo with the words life access\n",
      "Image 51: this is an image of a blue and white logo with the words life access\n",
      "Image 52: there is a black and white photo of a man in a black shirt\n",
      "Image 53: black and white photo of a woman wearing sunglasses and a jacket\n",
      "Image 54: this is a black and white photo of a man with a beard\n",
      "Image 55: this is a black and white photo of a man with a beard\n"
     ]
    }
   ],
   "source": [
    "#This is much better with this detailed description generation.!!!!!!!\n",
    "#Generate a detailed image description\n",
    "def generate_detailed_description(image: Image.Image) -> str:\n",
    "    \"\"\"Generate a detailed description for a given image using the BLIP model.\"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=500, num_beams=5, early_stopping=True)\n",
    "    description = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return description\n",
    "\n",
    "# Generate detailed descriptions for all images\n",
    "detailed_descriptions = []\n",
    "for image in images:\n",
    "    description = generate_detailed_description(image)\n",
    "    detailed_descriptions.append(description)\n",
    "print(\"Generated Detailed Descriptions:\")\n",
    "for i, description in enumerate(detailed_descriptions):\n",
    "    print(f\"Image {i}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcae91",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m detailed_descriptions = []\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     description = \u001b[43mgenerate_detailed_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     detailed_descriptions.append(description)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated Detailed Descriptions:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgenerate_detailed_description\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate a detailed description for a given image using the BLIP model.\"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m inputs = processor(images=image, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m description = processor.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m description\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip.py:1069\u001b[39m, in \u001b[36mBlipForConditionalGeneration.generate\u001b[39m\u001b[34m(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[33;03mOverrides *generate* function to be able to use the model as a conditional generator\u001b[39;00m\n\u001b[32m   1038\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1065\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1068\u001b[39m batch_size = pixel_values.shape[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m vision_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1074\u001b[39m image_embeds = vision_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1076\u001b[39m image_attention_mask = torch.ones(image_embeds.size()[:-\u001b[32m1\u001b[39m], dtype=torch.long, device=image_embeds.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip.py:613\u001b[39m, in \u001b[36mBlipVisionModel.forward\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify pixel_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    615\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m    616\u001b[39m     inputs_embeds=hidden_states,\n\u001b[32m    617\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    618\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    619\u001b[39m     return_dict=return_dict,\n\u001b[32m    620\u001b[39m )\n\u001b[32m    622\u001b[39m last_hidden_state = encoder_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\blip\\modeling_blip.py:270\u001b[39m, in \u001b[36mBlipVisionEmbeddings.forward\u001b[39m\u001b[34m(self, pixel_values, interpolate_pos_encoding)\u001b[39m\n\u001b[32m    268\u001b[39m batch_size, _, height, width = pixel_values.shape\n\u001b[32m    269\u001b[39m target_dtype = \u001b[38;5;28mself\u001b[39m.patch_embedding.weight.dtype\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m patch_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape = [*, width, grid, grid]\u001b[39;00m\n\u001b[32m    271\u001b[39m patch_embeds = patch_embeds.flatten(\u001b[32m2\u001b[39m).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    272\u001b[39m class_embeds = \u001b[38;5;28mself\u001b[39m.class_embedding.expand(batch_size, \u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m).to(target_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\T440\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")  # Move model to CUDA\n",
    "# Move input tensors to CUDA as well\n",
    "inputs = processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "#This is much better with this detailed description generation.!!!!!!!\n",
    "#Generate a detailed image description\n",
    "def generate_detailed_description(image: Image.Image) -> str:\n",
    "    \"\"\"Generate a detailed description for a given image using the BLIP model.\"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=500, num_beams=5, early_stopping=True)\n",
    "    description = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return description\n",
    "\n",
    "# Generate detailed descriptions for all images\n",
    "detailed_descriptions = []\n",
    "for image in images:\n",
    "    description = generate_detailed_description(image)\n",
    "    detailed_descriptions.append(description)\n",
    "print(\"Generated Detailed Descriptions:\")\n",
    "for i, description in enumerate(detailed_descriptions):\n",
    "    print(f\"Image {i}: {description}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
